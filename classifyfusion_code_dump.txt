# ./tests/__init__.py
"""Test suite for textclassify package."""



# ./textclassify/__init__.py
"""
TextClassify: A comprehensive text classification package supporting LLMs and traditional ML models.

This package provides multi-class and multi-label text classification capabilities using:
- LLM providers: OpenAI, Claude, Gemini, DeepSeek
- Traditional ML models: RoBERTa (optional)
- Ensemble methods for optimized performance
"""

__version__ = "0.1.0"
__author__ = "TextClassify Team"
__email__ = "contact@textclassify.com"

# Core imports (always available)
from .core.base import BaseClassifier
from .core.types import (
    ClassificationResult, ClassificationType, ModelType, LLMProvider,
    TrainingData, ModelConfig, EnsembleConfig
)
from .core.exceptions import TextClassifyError, ModelNotFoundError, ConfigurationError, APIError, EnsembleError

# LLM Classifiers (require API keys but no additional dependencies)
from .llm.openai_classifier import OpenAIClassifier
from .llm.claude_classifier import ClaudeClassifier
from .llm.gemini_classifier import GeminiClassifier
from .llm.deepseek_classifier import DeepSeekClassifier

# Traditional ML Classifiers (optional - require transformers/torch)
try:
    from .ml.roberta_classifier import RoBERTaClassifier
    _HAS_ML = True
except ImportError:
    _HAS_ML = False
    RoBERTaClassifier = None

# Ensemble Methods
from .ensemble.voting import VotingEnsemble
from .ensemble.weighted import WeightedEnsemble
from .ensemble.routing import ClassRoutingEnsemble

# Configuration
from .config.settings import Config
from .config.api_keys import APIKeyManager

# Base exports
__all__ = [
    # Core
    "BaseClassifier",
    "ClassificationResult",
    "ClassificationType",
    "ModelType", 
    "LLMProvider",
    "TrainingData",
    "ModelConfig",
    "EnsembleConfig",
    "TextClassifyError",
    "ModelNotFoundError",
    "ConfigurationError",
    "APIError",
    "EnsembleError",
    
    # LLM Classifiers
    "OpenAIClassifier",
    "ClaudeClassifier", 
    "GeminiClassifier",
    "DeepSeekClassifier",
    
    # Ensemble Methods
    "VotingEnsemble",
    "WeightedEnsemble",
    "ClassRoutingEnsemble",
    
    # Configuration
    "Config",
    "APIKeyManager",
    
    # Version
    "__version__",
]

# Add ML classifiers to exports if available
if _HAS_ML:
    __all__.append("RoBERTaClassifier")


def get_available_features():
    """Get information about available features based on installed dependencies.
    
    Returns:
        dict: Dictionary with feature availability information
    """
    features = {
        "llm_classifiers": True,
        "ensemble_methods": True,
        "configuration": True,
        "traditional_ml": _HAS_ML,
    }
    
    # Check for optional dependencies
    try:
        import yaml
        features["yaml_config"] = True
    except ImportError:
        features["yaml_config"] = False
    
    return features


def check_dependencies():
    """Check and report on package dependencies.
    
    Prints information about available and missing dependencies.
    """
    print("TextClassify Dependency Check")
    print("=" * 30)
    
    # Core dependencies
    core_deps = ["aiohttp", "requests", "numpy", "pandas"]
    print("\nCore Dependencies:")
    for dep in core_deps:
        try:
            __import__(dep)
            print(f"  ✓ {dep}")
        except ImportError:
            print(f"  ✗ {dep} (missing)")
    
    # Optional dependencies
    optional_deps = {
        "torch": "Required for RoBERTa classifier",
        "transformers": "Required for RoBERTa classifier", 
        "scikit-learn": "Required for ML utilities",
        "yaml": "Required for YAML configuration files"
    }
    
    print("\nOptional Dependencies:")
    for dep, description in optional_deps.items():
        try:
            __import__(dep)
            print(f"  ✓ {dep} - {description}")
        except ImportError:
            print(f"  ✗ {dep} - {description}")
    
    # Feature availability
    features = get_available_features()
    print("\nAvailable Features:")
    for feature, available in features.items():
        status = "✓" if available else "✗"
        print(f"  {status} {feature.replace('_', ' ').title()}")
    
    if not features["traditional_ml"]:
        print("\nTo enable traditional ML features, install:")
        print("  pip install transformers torch scikit-learn")
    
    if not features["yaml_config"]:
        print("\nTo enable YAML configuration support, install:")
        print("  pip install pyyaml")



# ./textclassify/config/__init__.py
"""Configuration management module."""

from .settings import Config, load_config, save_config
from .api_keys import APIKeyManager

__all__ = [
    "Config",
    "load_config",
    "save_config",
    "APIKeyManager",
]



# ./textclassify/config/api_keys.py
"""API key management for LLM providers."""

import os
import json
from pathlib import Path
from typing import Dict, Optional
import getpass

from ..core.exceptions import ConfigurationError


class APIKeyManager:
    """Manager for API keys with secure storage and retrieval."""
    
    def __init__(self, key_file: Optional[str] = None):
        """Initialize API key manager.
        
        Args:
            key_file: Path to API key file (optional)
        """
        self.key_file = key_file or self._get_default_key_file()
        self._keys = {}
        self._load_keys()
    
    def _get_default_key_file(self) -> str:
        """Get default API key file path.
        
        Returns:
            Default path for API key file
        """
        home_dir = Path.home()
        config_dir = home_dir / '.textclassify'
        config_dir.mkdir(exist_ok=True)
        return str(config_dir / 'api_keys.json')
    
    def _load_keys(self) -> None:
        """Load API keys from file."""
        try:
            if os.path.exists(self.key_file):
                with open(self.key_file, 'r') as f:
                    self._keys = json.load(f)
            else:
                self._keys = {}
        except Exception as e:
            print(f"Warning: Failed to load API keys: {str(e)}")
            self._keys = {}
    
    def _save_keys(self) -> None:
        """Save API keys to file."""
        try:
            # Ensure directory exists
            os.makedirs(os.path.dirname(self.key_file), exist_ok=True)
            
            with open(self.key_file, 'w') as f:
                json.dump(self._keys, f, indent=2)
            
            # Set restrictive permissions (Unix-like systems only)
            try:
                os.chmod(self.key_file, 0o600)
            except (OSError, AttributeError):
                pass  # Windows or permission error
                
        except Exception as e:
            raise ConfigurationError(f"Failed to save API keys: {str(e)}")
    
    def set_key(self, provider: str, api_key: str) -> None:
        """Set API key for a provider.
        
        Args:
            provider: Provider name (e.g., 'openai', 'claude')
            api_key: API key string
        """
        if not api_key or not api_key.strip():
            raise ConfigurationError("API key cannot be empty")
        
        self._keys[provider.lower()] = api_key.strip()
        self._save_keys()
    
    def get_key(self, provider: str) -> Optional[str]:
        """Get API key for a provider.
        
        Args:
            provider: Provider name
            
        Returns:
            API key or None if not found
        """
        # First check stored keys
        key = self._keys.get(provider.lower())
        
        if key:
            return key
        
        # Then check environment variables
        env_var_names = [
            f"{provider.upper()}_API_KEY",
            f"TEXTCLASSIFY_{provider.upper()}_API_KEY",
            f"{provider.upper()}_KEY"
        ]
        
        for env_var in env_var_names:
            key = os.getenv(env_var)
            if key:
                return key.strip()
        
        return None
    
    def remove_key(self, provider: str) -> bool:
        """Remove API key for a provider.
        
        Args:
            provider: Provider name
            
        Returns:
            True if key was removed, False if not found
        """
        provider_lower = provider.lower()
        if provider_lower in self._keys:
            del self._keys[provider_lower]
            self._save_keys()
            return True
        return False
    
    def list_providers(self) -> list:
        """List providers with stored API keys.
        
        Returns:
            List of provider names
        """
        return list(self._keys.keys())
    
    def has_key(self, provider: str) -> bool:
        """Check if API key exists for a provider.
        
        Args:
            provider: Provider name
            
        Returns:
            True if key exists, False otherwise
        """
        return self.get_key(provider) is not None
    
    def prompt_for_key(self, provider: str, save: bool = True) -> Optional[str]:
        """Prompt user for API key.
        
        Args:
            provider: Provider name
            save: Whether to save the key after prompting
            
        Returns:
            API key or None if cancelled
        """
        try:
            print(f"\nAPI key required for {provider}")
            print(f"You can also set the environment variable {provider.upper()}_API_KEY")
            
            api_key = getpass.getpass(f"Enter {provider} API key (input hidden): ")
            
            if api_key and api_key.strip():
                api_key = api_key.strip()
                if save:
                    self.set_key(provider, api_key)
                    print(f"API key saved for {provider}")
                return api_key
            else:
                print("No API key provided")
                return None
                
        except (KeyboardInterrupt, EOFError):
            print("\nCancelled")
            return None
    
    def validate_key_format(self, provider: str, api_key: str) -> bool:
        """Validate API key format for a provider.
        
        Args:
            provider: Provider name
            api_key: API key to validate
            
        Returns:
            True if format appears valid, False otherwise
        """
        if not api_key or not api_key.strip():
            return False
        
        api_key = api_key.strip()
        provider_lower = provider.lower()
        
        # Basic format validation
        if provider_lower == 'openai':
            return api_key.startswith('sk-') and len(api_key) > 20
        elif provider_lower == 'claude':
            return api_key.startswith('sk-ant-') and len(api_key) > 20
        elif provider_lower == 'gemini':
            return len(api_key) > 20  # Google API keys vary in format
        elif provider_lower == 'deepseek':
            return api_key.startswith('sk-') and len(api_key) > 20
        else:
            # For unknown providers, just check it's not empty
            return len(api_key) > 5
    
    def setup_provider(self, provider: str) -> bool:
        """Interactive setup for a provider's API key.
        
        Args:
            provider: Provider name
            
        Returns:
            True if setup successful, False otherwise
        """
        print(f"\nSetting up {provider} API key...")
        
        # Check if key already exists
        existing_key = self.get_key(provider)
        if existing_key:
            print(f"API key for {provider} already exists")
            response = input("Do you want to replace it? (y/N): ").strip().lower()
            if response not in ['y', 'yes']:
                return True
        
        # Prompt for new key
        api_key = self.prompt_for_key(provider, save=False)
        
        if not api_key:
            return False
        
        # Validate format
        if not self.validate_key_format(provider, api_key):
            print(f"Warning: API key format for {provider} appears invalid")
            response = input("Do you want to save it anyway? (y/N): ").strip().lower()
            if response not in ['y', 'yes']:
                return False
        
        # Save the key
        try:
            self.set_key(provider, api_key)
            print(f"API key for {provider} saved successfully")
            return True
        except Exception as e:
            print(f"Failed to save API key: {str(e)}")
            return False
    
    def get_all_keys(self) -> Dict[str, str]:
        """Get all stored API keys.
        
        Returns:
            Dictionary of provider -> API key
        """
        all_keys = {}
        
        # Get stored keys
        all_keys.update(self._keys)
        
        # Check environment variables for missing keys
        providers = ['openai', 'claude', 'gemini', 'deepseek']
        for provider in providers:
            if provider not in all_keys:
                env_key = self.get_key(provider)
                if env_key:
                    all_keys[provider] = env_key
        
        return all_keys
    
    def clear_all_keys(self) -> None:
        """Clear all stored API keys."""
        self._keys = {}
        self._save_keys()
        print("All API keys cleared")



# ./textclassify/config/settings.py
"""Configuration settings management."""

import json
import yaml
import os
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

from ..core.types import ModelConfig, EnsembleConfig, ClassificationType, ModelType, LLMProvider
from ..core.exceptions import ConfigurationError


class Config:
    """Main configuration class for textclassify package."""
    
    def __init__(self, config_path: Optional[str] = None):
        """Initialize configuration.
        
        Args:
            config_path: Path to configuration file (optional)
        """
        self.config_path = config_path
        self.data = {}
        
        # Default configuration
        self._set_defaults()
        
        # Load from file if provided
        if config_path and os.path.exists(config_path):
            self.load(config_path)
    
    def _set_defaults(self):
        """Set default configuration values."""
        self.data = {
            "general": {
                "default_classification_type": "multi_class",
                "default_batch_size": 32,
                "default_timeout": 30.0,
                "default_max_retries": 3,
                "enable_caching": True,
                "cache_dir": "~/.textclassify/cache",
                "log_level": "INFO"
            },
            "llm": {
                "default_provider": "openai",
                "default_models": {
                    "openai": "gpt-3.5-turbo",
                    "claude": "claude-3-haiku-20240307",
                    "gemini": "gemini-1.5-flash",
                    "deepseek": "deepseek-chat"
                },
                "default_parameters": {
                    "temperature": 0.1,
                    "max_tokens": 150,
                    "max_examples": 5
                }
            },
            "ml": {
                "default_model": "roberta-base",
                "default_parameters": {
                    "max_length": 512,
                    "batch_size": 16,
                    "learning_rate": 2e-5,
                    "num_epochs": 3,
                    "warmup_steps": 0,
                    "weight_decay": 0.01
                },
                "preprocessing": {
                    "lowercase": True,
                    "remove_punctuation": False,
                    "remove_numbers": False,
                    "remove_extra_whitespace": True,
                    "min_length": 1,
                    "max_length": None
                }
            },
            "ensemble": {
                "default_method": "voting",
                "default_voting_strategy": "majority",
                "default_threshold": 0.5,
                "require_all_models": False
            },
            "api_keys": {
                "openai": None,
                "claude": None,
                "gemini": None,
                "deepseek": None
            }
        }
    
    def load(self, config_path: str) -> None:
        """Load configuration from file.
        
        Args:
            config_path: Path to configuration file
            
        Raises:
            ConfigurationError: If loading fails
        """
        try:
            config_path = Path(config_path).expanduser()
            
            if not config_path.exists():
                raise ConfigurationError(f"Configuration file not found: {config_path}")
            
            with open(config_path, 'r') as f:
                if config_path.suffix.lower() in ['.yaml', '.yml']:
                    loaded_data = yaml.safe_load(f)
                elif config_path.suffix.lower() == '.json':
                    loaded_data = json.load(f)
                else:
                    raise ConfigurationError(f"Unsupported configuration file format: {config_path.suffix}")
            
            # Merge with defaults
            self._merge_config(self.data, loaded_data)
            self.config_path = str(config_path)
            
        except Exception as e:
            raise ConfigurationError(f"Failed to load configuration: {str(e)}")
    
    def save(self, config_path: Optional[str] = None) -> None:
        """Save configuration to file.
        
        Args:
            config_path: Path to save configuration (optional, uses loaded path if not provided)
            
        Raises:
            ConfigurationError: If saving fails
        """
        save_path = config_path or self.config_path
        
        if not save_path:
            raise ConfigurationError("No configuration path specified")
        
        try:
            save_path = Path(save_path).expanduser()
            save_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(save_path, 'w') as f:
                if save_path.suffix.lower() in ['.yaml', '.yml']:
                    yaml.dump(self.data, f, default_flow_style=False, indent=2)
                elif save_path.suffix.lower() == '.json':
                    json.dump(self.data, f, indent=2)
                else:
                    raise ConfigurationError(f"Unsupported configuration file format: {save_path.suffix}")
            
            self.config_path = str(save_path)
            
        except Exception as e:
            raise ConfigurationError(f"Failed to save configuration: {str(e)}")
    
    def _merge_config(self, base: Dict, update: Dict) -> None:
        """Recursively merge configuration dictionaries.
        
        Args:
            base: Base configuration dictionary
            update: Update configuration dictionary
        """
        for key, value in update.items():
            if key in base and isinstance(base[key], dict) and isinstance(value, dict):
                self._merge_config(base[key], value)
            else:
                base[key] = value
    
    def get(self, key: str, default: Any = None) -> Any:
        """Get configuration value using dot notation.
        
        Args:
            key: Configuration key (e.g., 'llm.default_provider')
            default: Default value if key not found
            
        Returns:
            Configuration value
        """
        keys = key.split('.')
        value = self.data
        
        for k in keys:
            if isinstance(value, dict) and k in value:
                value = value[k]
            else:
                return default
        
        return value
    
    def set(self, key: str, value: Any) -> None:
        """Set configuration value using dot notation.
        
        Args:
            key: Configuration key (e.g., 'llm.default_provider')
            value: Value to set
        """
        keys = key.split('.')
        config = self.data
        
        for k in keys[:-1]:
            if k not in config:
                config[k] = {}
            config = config[k]
        
        config[keys[-1]] = value
    
    def create_model_config(
        self,
        model_name: str,
        model_type: Union[str, ModelType],
        provider: Optional[Union[str, LLMProvider]] = None,
        **kwargs
    ) -> ModelConfig:
        """Create a ModelConfig from the current configuration.
        
        Args:
            model_name: Name of the model
            model_type: Type of the model
            provider: LLM provider (for LLM models)
            **kwargs: Additional parameters
            
        Returns:
            ModelConfig instance
        """
        if isinstance(model_type, str):
            model_type = ModelType(model_type)
        
        # Get default parameters based on model type
        if model_type == ModelType.LLM:
            default_params = self.get('llm.default_parameters', {}).copy()
            if provider:
                if isinstance(provider, str):
                    provider = LLMProvider(provider)
                api_key = self.get(f'api_keys.{provider.value}')
            else:
                api_key = None
        else:
            default_params = self.get('ml.default_parameters', {}).copy()
            api_key = None
        
        # Merge with provided parameters
        default_params.update(kwargs)
        
        return ModelConfig(
            model_name=model_name,
            model_type=model_type,
            parameters=default_params,
            api_key=api_key,
            batch_size=self.get('general.default_batch_size', 32),
            max_retries=self.get('general.default_max_retries', 3),
            timeout=self.get('general.default_timeout', 30.0),
            enable_caching=self.get('general.enable_caching', True)
        )
    
    def create_ensemble_config(
        self,
        models: List[ModelConfig],
        ensemble_method: str = None,
        **kwargs
    ) -> EnsembleConfig:
        """Create an EnsembleConfig from the current configuration.
        
        Args:
            models: List of model configurations
            ensemble_method: Ensemble method to use
            **kwargs: Additional parameters
            
        Returns:
            EnsembleConfig instance
        """
        if ensemble_method is None:
            ensemble_method = self.get('ensemble.default_method', 'voting')
        
        return EnsembleConfig(
            models=models,
            ensemble_method=ensemble_method,
            require_all_models=self.get('ensemble.require_all_models', False),
            **kwargs
        )
    
    def get_cache_dir(self) -> Path:
        """Get the cache directory path.
        
        Returns:
            Path to cache directory
        """
        cache_dir = self.get('general.cache_dir', '~/.textclassify/cache')
        return Path(cache_dir).expanduser()
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert configuration to dictionary.
        
        Returns:
            Configuration as dictionary
        """
        return self.data.copy()


def load_config(config_path: str) -> Config:
    """Load configuration from file.
    
    Args:
        config_path: Path to configuration file
        
    Returns:
        Config instance
    """
    return Config(config_path)


def save_config(config: Config, config_path: str) -> None:
    """Save configuration to file.
    
    Args:
        config: Configuration instance
        config_path: Path to save configuration
    """
    config.save(config_path)


def create_default_config(config_path: str) -> Config:
    """Create a default configuration file.
    
    Args:
        config_path: Path to save the default configuration
        
    Returns:
        Config instance with defaults
    """
    config = Config()
    config.save(config_path)
    return config



# ./textclassify/core/__init__.py
"""Core module containing base classes, types, and exceptions."""

from .base import BaseClassifier
from .types import ClassificationResult, ClassificationType, ModelType
from .exceptions import TextClassifyError, ModelNotFoundError, ConfigurationError

__all__ = [
    "BaseClassifier",
    "ClassificationResult", 
    "ClassificationType",
    "ModelType",
    "TextClassifyError",
    "ModelNotFoundError",
    "ConfigurationError",
]



# ./textclassify/core/base.py
"""Base classifier interface and abstract classes."""

import time
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional, Union

from .types import ClassificationResult, ClassificationType, ModelConfig, ModelType, TrainingData
from .exceptions import ValidationError, PredictionError


class BaseClassifier(ABC):
    """Abstract base class for all text classifiers."""
    
    def __init__(self, config: ModelConfig):
        """Initialize the classifier with configuration.
        
        Args:
            config: Model configuration containing parameters and settings
        """
        self.config = config
        self.is_trained = False
        self.classes_ = None
        self.classification_type = None
        
    @abstractmethod
    def fit(self, training_data: TrainingData) -> None:
        """Train the classifier on the provided data.
        
        Args:
            training_data: Training data containing texts and labels
            
        Raises:
            ModelTrainingError: If training fails
        """
        pass
    
    @abstractmethod
    def predict(self, texts: List[str]) -> ClassificationResult:
        """Predict labels for the given texts.
        
        Args:
            texts: List of texts to classify
            
        Returns:
            ClassificationResult containing predictions and metadata
            
        Raises:
            PredictionError: If prediction fails
            ValidationError: If input validation fails
        """
        pass
    
    @abstractmethod
    def predict_proba(self, texts: List[str]) -> ClassificationResult:
        """Predict class probabilities for the given texts.
        
        Args:
            texts: List of texts to classify
            
        Returns:
            ClassificationResult containing predictions, probabilities and metadata
            
        Raises:
            PredictionError: If prediction fails
            ValidationError: If input validation fails
        """
        pass
    
    def validate_input(self, texts: List[str]) -> None:
        """Validate input texts.
        
        Args:
            texts: List of texts to validate
            
        Raises:
            ValidationError: If validation fails
        """
        if not texts:
            raise ValidationError("Input texts cannot be empty")
        
        if not all(isinstance(text, str) for text in texts):
            raise ValidationError("All inputs must be strings")
        
        if not all(text.strip() for text in texts):
            raise ValidationError("Input texts cannot be empty or whitespace only")
    
    def _create_result(
        self,
        predictions: List[Union[str, List[str]]],
        probabilities: Optional[List[Dict[str, float]]] = None,
        confidence_scores: Optional[List[float]] = None,
        processing_time: Optional[float] = None,
        **metadata
    ) -> ClassificationResult:
        """Create a ClassificationResult with standard metadata.
        
        Args:
            predictions: Predicted labels
            probabilities: Class probabilities (optional)
            confidence_scores: Confidence scores (optional)
            processing_time: Time taken for processing (optional)
            **metadata: Additional metadata
            
        Returns:
            ClassificationResult with populated metadata
        """
        return ClassificationResult(
            predictions=predictions,
            probabilities=probabilities,
            confidence_scores=confidence_scores,
            model_name=self.config.model_name,
            model_type=self.config.model_type,
            classification_type=self.classification_type,
            processing_time=processing_time,
            metadata=metadata
        )
    
    def _time_operation(self, operation_func, *args, **kwargs):
        """Time an operation and return result with timing.
        
        Args:
            operation_func: Function to time
            *args: Arguments for the function
            **kwargs: Keyword arguments for the function
            
        Returns:
            Tuple of (result, processing_time)
        """
        start_time = time.time()
        try:
            result = operation_func(*args, **kwargs)
            processing_time = time.time() - start_time
            return result, processing_time
        except Exception as e:
            processing_time = time.time() - start_time
            raise PredictionError(
                f"Operation failed after {processing_time:.2f}s: {str(e)}",
                model_name=self.config.model_name
            )
    
    @property
    def model_info(self) -> Dict[str, Any]:
        """Get information about the model.
        
        Returns:
            Dictionary containing model information
        """
        return {
            "model_name": self.config.model_name,
            "model_type": self.config.model_type.value,
            "is_trained": self.is_trained,
            "classification_type": self.classification_type.value if self.classification_type else None,
            "classes": self.classes_,
            "config": self.config.parameters
        }


class AsyncBaseClassifier(BaseClassifier):
    """Base class for asynchronous classifiers (primarily for LLM-based models)."""
    
    @abstractmethod
    async def predict_async(self, texts: List[str]) -> ClassificationResult:
        """Asynchronously predict labels for the given texts.
        
        Args:
            texts: List of texts to classify
            
        Returns:
            ClassificationResult containing predictions and metadata
            
        Raises:
            PredictionError: If prediction fails
            ValidationError: If input validation fails
        """
        pass
    
    @abstractmethod
    async def predict_proba_async(self, texts: List[str]) -> ClassificationResult:
        """Asynchronously predict class probabilities for the given texts.
        
        Args:
            texts: List of texts to classify
            
        Returns:
            ClassificationResult containing predictions, probabilities and metadata
            
        Raises:
            PredictionError: If prediction fails
            ValidationError: If input validation fails
        """
        pass



# ./textclassify/core/exceptions.py
"""Custom exceptions for the textclassify package."""


class TextClassifyError(Exception):
    """Base exception for all textclassify errors."""
    
    def __init__(self, message: str, error_code: str = None):
        super().__init__(message)
        self.message = message
        self.error_code = error_code


class ModelNotFoundError(TextClassifyError):
    """Raised when a requested model is not found or not available."""
    
    def __init__(self, model_name: str):
        message = f"Model '{model_name}' not found or not available"
        super().__init__(message, "MODEL_NOT_FOUND")
        self.model_name = model_name


class ConfigurationError(TextClassifyError):
    """Raised when there's an error in configuration."""
    
    def __init__(self, message: str, config_key: str = None):
        super().__init__(message, "CONFIGURATION_ERROR")
        self.config_key = config_key


class APIError(TextClassifyError):
    """Raised when there's an error with external API calls."""
    
    def __init__(self, message: str, provider: str = None, status_code: int = None):
        super().__init__(message, "API_ERROR")
        self.provider = provider
        self.status_code = status_code


class ModelTrainingError(TextClassifyError):
    """Raised when there's an error during model training."""
    
    def __init__(self, message: str, model_name: str = None):
        super().__init__(message, "MODEL_TRAINING_ERROR")
        self.model_name = model_name


class PredictionError(TextClassifyError):
    """Raised when there's an error during prediction."""
    
    def __init__(self, message: str, model_name: str = None):
        super().__init__(message, "PREDICTION_ERROR")
        self.model_name = model_name


class ValidationError(TextClassifyError):
    """Raised when input validation fails."""
    
    def __init__(self, message: str, field_name: str = None):
        super().__init__(message, "VALIDATION_ERROR")
        self.field_name = field_name


class EnsembleError(TextClassifyError):
    """Raised when there's an error in ensemble operations."""
    
    def __init__(self, message: str, ensemble_method: str = None):
        super().__init__(message, "ENSEMBLE_ERROR")
        self.ensemble_method = ensemble_method



# ./textclassify/core/types.py
"""Core types and data structures for text classification."""

from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional, Union


class ClassificationType(Enum):
    """Type of classification task."""
    MULTI_CLASS = "multi_class"  # Single label per text (mutually exclusive)
    MULTI_LABEL = "multi_label"  # Multiple labels per text (non-exclusive)


class ModelType(Enum):
    """Type of model used for classification."""
    LLM = "llm"
    TRADITIONAL_ML = "traditional_ml"
    ENSEMBLE = "ensemble"


class LLMProvider(Enum):
    """Supported LLM providers."""
    OPENAI = "openai"
    CLAUDE = "claude"
    GEMINI = "gemini"
    DEEPSEEK = "deepseek"


@dataclass
class ClassificationResult:
    """Result of a classification operation."""
    
    # Core prediction results
    predictions: List[Union[str, List[str]]]  # Single label or list of labels per text
    probabilities: Optional[List[Dict[str, float]]] = None  # Class probabilities per text
    confidence_scores: Optional[List[float]] = None  # Overall confidence per text
    
    # Metadata
    model_name: Optional[str] = None
    model_type: Optional[ModelType] = None
    classification_type: Optional[ClassificationType] = None
    processing_time: Optional[float] = None
    
    # Additional metadata
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        """Validate the result after initialization."""
        if not self.predictions:
            raise ValueError("Predictions cannot be empty")
        
        # Validate probabilities if provided
        if self.probabilities is not None:
            if len(self.probabilities) != len(self.predictions):
                raise ValueError("Probabilities length must match predictions length")
        
        # Validate confidence scores if provided
        if self.confidence_scores is not None:
            if len(self.confidence_scores) != len(self.predictions):
                raise ValueError("Confidence scores length must match predictions length")


@dataclass
class TrainingData:
    """Training data for classification models."""
    
    texts: List[str]
    labels: Union[List[str], List[List[str]]]  # Single or multiple labels per text
    classification_type: ClassificationType
    
    # Optional metadata
    text_ids: Optional[List[str]] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        """Validate training data after initialization."""
        if len(self.texts) != len(self.labels):
            raise ValueError("Texts and labels must have the same length")
        
        if not self.texts:
            raise ValueError("Training data cannot be empty")
        
        # Validate label format based on classification type
        if self.classification_type == ClassificationType.MULTI_CLASS:
            if not all(isinstance(label, str) for label in self.labels):
                raise ValueError("Multi-class labels must be strings")
        elif self.classification_type == ClassificationType.MULTI_LABEL:
            if not all(isinstance(label, list) for label in self.labels):
                raise ValueError("Multi-label labels must be lists of strings")


@dataclass
class ModelConfig:
    """Configuration for a classification model."""
    
    model_name: str
    model_type: ModelType
    
    # Model-specific parameters
    parameters: Dict[str, Any] = field(default_factory=dict)
    
    # API configuration (for LLM models)
    api_key: Optional[str] = None
    api_base: Optional[str] = None
    
    # Training configuration (for ML models)
    training_config: Dict[str, Any] = field(default_factory=dict)
    
    # Caching and performance
    enable_caching: bool = True
    batch_size: int = 32
    max_retries: int = 3
    timeout: float = 30.0


@dataclass
class EnsembleConfig:
    """Configuration for ensemble methods."""
    
    models: List[ModelConfig]
    ensemble_method: str  # "voting", "weighted", "routing"
    
    # Method-specific parameters
    weights: Optional[List[float]] = None  # For weighted ensemble
    routing_rules: Optional[Dict[str, str]] = None  # For class routing
    
    # Ensemble behavior
    require_all_models: bool = False  # Whether all models must succeed
    fallback_model: Optional[str] = None  # Fallback if primary models fail



# ./textclassify/ensemble/__init__.py
"""Ensemble methods for combining multiple classifiers."""

from .base import BaseEnsemble
from .voting import VotingEnsemble
from .weighted import WeightedEnsemble
from .routing import ClassRoutingEnsemble

__all__ = [
    "BaseEnsemble",
    "VotingEnsemble",
    "WeightedEnsemble",
    "ClassRoutingEnsemble",
]



# ./textclassify/ensemble/base.py
"""Base class for ensemble methods."""

import asyncio
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional, Union

from ..core.base import BaseClassifier, AsyncBaseClassifier
from ..core.types import ClassificationResult, ClassificationType, ModelType, TrainingData, EnsembleConfig
from ..core.exceptions import EnsembleError, PredictionError, ValidationError


class BaseEnsemble(BaseClassifier):
    """Base class for ensemble methods."""
    
    def __init__(self, ensemble_config: EnsembleConfig):
        """Initialize ensemble with configuration.
        
        Args:
            ensemble_config: Configuration for the ensemble
        """
        # Create a dummy config for the base class
        from ..core.types import ModelConfig
        dummy_config = ModelConfig(
            model_name=f"ensemble_{ensemble_config.ensemble_method}",
            model_type=ModelType.ENSEMBLE
        )
        
        super().__init__(dummy_config)
        self.ensemble_config = ensemble_config
        self.models = []
        self.model_names = []
        
        # Validate ensemble configuration
        if not ensemble_config.models:
            raise EnsembleError("Ensemble must have at least one model", ensemble_config.ensemble_method)
    
    def add_model(self, model: BaseClassifier, name: Optional[str] = None) -> None:
        """Add a model to the ensemble.
        
        Args:
            model: Classifier to add to the ensemble
            name: Optional name for the model
        """
        if not isinstance(model, BaseClassifier):
            raise EnsembleError("Model must be a BaseClassifier instance", self.ensemble_config.ensemble_method)
        
        self.models.append(model)
        model_name = name or f"model_{len(self.models)}"
        self.model_names.append(model_name)
    
    def fit(self, training_data: TrainingData) -> None:
        """Fit all models in the ensemble.
        
        Args:
            training_data: Training data for all models
        """
        if not self.models:
            raise EnsembleError("No models added to ensemble", self.ensemble_config.ensemble_method)
        
        self.classification_type = training_data.classification_type
        
        # Collect all unique classes from all models after training
        all_classes = set()
        
        for i, model in enumerate(self.models):
            try:
                model.fit(training_data)
                if model.classes_:
                    all_classes.update(model.classes_)
            except Exception as e:
                if self.ensemble_config.require_all_models:
                    raise EnsembleError(
                        f"Failed to train model {self.model_names[i]}: {str(e)}",
                        self.ensemble_config.ensemble_method
                    )
                else:
                    print(f"Warning: Failed to train model {self.model_names[i]}: {str(e)}")
        
        self.classes_ = list(all_classes)
        self.is_trained = True
    
    def predict(self, texts: List[str]) -> ClassificationResult:
        """Predict using ensemble method.
        
        Args:
            texts: List of texts to classify
            
        Returns:
            ClassificationResult with ensemble predictions
        """
        self.validate_input(texts)
        
        if not self.is_trained:
            raise PredictionError("Ensemble must be trained before prediction", self.config.model_name)
        
        # Get predictions from all models
        model_results = self._get_model_predictions(texts, with_probabilities=False)
        
        # Combine predictions using ensemble method
        ensemble_predictions = self._combine_predictions(model_results, texts)
        
        return self._create_result(predictions=ensemble_predictions)
    
    def predict_proba(self, texts: List[str]) -> ClassificationResult:
        """Predict probabilities using ensemble method.
        
        Args:
            texts: List of texts to classify
            
        Returns:
            ClassificationResult with ensemble predictions and probabilities
        """
        self.validate_input(texts)
        
        if not self.is_trained:
            raise PredictionError("Ensemble must be trained before prediction", self.config.model_name)
        
        # Get predictions with probabilities from all models
        model_results = self._get_model_predictions(texts, with_probabilities=True)
        
        # Combine predictions and probabilities using ensemble method
        ensemble_predictions, ensemble_probabilities, ensemble_confidence = self._combine_predictions_with_probabilities(
            model_results, texts
        )
        
        return self._create_result(
            predictions=ensemble_predictions,
            probabilities=ensemble_probabilities,
            confidence_scores=ensemble_confidence
        )
    
    def _get_model_predictions(self, texts: List[str], with_probabilities: bool = False) -> List[ClassificationResult]:
        """Get predictions from all models.
        
        Args:
            texts: List of texts to classify
            with_probabilities: Whether to get probabilities
            
        Returns:
            List of ClassificationResult from each model
        """
        model_results = []
        
        for i, model in enumerate(self.models):
            try:
                if with_probabilities:
                    result = model.predict_proba(texts)
                else:
                    result = model.predict(texts)
                model_results.append(result)
            except Exception as e:
                if self.ensemble_config.require_all_models:
                    raise EnsembleError(
                        f"Failed to get predictions from model {self.model_names[i]}: {str(e)}",
                        self.ensemble_config.ensemble_method
                    )
                else:
                    print(f"Warning: Failed to get predictions from model {self.model_names[i]}: {str(e)}")
                    # Add empty result as placeholder
                    empty_predictions = [""] * len(texts) if self.classification_type == ClassificationType.MULTI_CLASS else [[] for _ in texts]
                    empty_result = ClassificationResult(predictions=empty_predictions)
                    model_results.append(empty_result)
        
        return model_results
    
    @abstractmethod
    def _combine_predictions(self, model_results: List[ClassificationResult], texts: List[str]) -> List[Union[str, List[str]]]:
        """Combine predictions from multiple models.
        
        Args:
            model_results: Results from all models
            texts: Original texts (for context)
            
        Returns:
            Combined predictions
        """
        pass
    
    @abstractmethod
    def _combine_predictions_with_probabilities(
        self, 
        model_results: List[ClassificationResult], 
        texts: List[str]
    ) -> tuple:
        """Combine predictions and probabilities from multiple models.
        
        Args:
            model_results: Results from all models
            texts: Original texts (for context)
            
        Returns:
            Tuple of (predictions, probabilities, confidence_scores)
        """
        pass
    
    def _handle_async_models(self, texts: List[str], with_probabilities: bool = False) -> List[ClassificationResult]:
        """Handle asynchronous models in the ensemble.
        
        Args:
            texts: List of texts to classify
            with_probabilities: Whether to get probabilities
            
        Returns:
            List of ClassificationResult from each model
        """
        async def get_async_predictions():
            tasks = []
            
            for model in self.models:
                if isinstance(model, AsyncBaseClassifier):
                    if with_probabilities:
                        task = model.predict_proba_async(texts)
                    else:
                        task = model.predict_async(texts)
                else:
                    # For synchronous models, wrap in async
                    if with_probabilities:
                        task = asyncio.create_task(asyncio.to_thread(model.predict_proba, texts))
                    else:
                        task = asyncio.create_task(asyncio.to_thread(model.predict, texts))
                
                tasks.append(task)
            
            return await asyncio.gather(*tasks, return_exceptions=True)
        
        # Run async predictions
        results = asyncio.run(get_async_predictions())
        
        # Handle exceptions
        model_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                if self.ensemble_config.require_all_models:
                    raise EnsembleError(
                        f"Failed to get predictions from model {self.model_names[i]}: {str(result)}",
                        self.ensemble_config.ensemble_method
                    )
                else:
                    print(f"Warning: Failed to get predictions from model {self.model_names[i]}: {str(result)}")
                    # Add empty result as placeholder
                    empty_predictions = [""] * len(texts) if self.classification_type == ClassificationType.MULTI_CLASS else [[] for _ in texts]
                    empty_result = ClassificationResult(predictions=empty_predictions)
                    model_results.append(empty_result)
            else:
                model_results.append(result)
        
        return model_results
    
    @property
    def model_info(self) -> Dict[str, Any]:
        """Get ensemble information."""
        info = super().model_info
        info.update({
            "ensemble_method": self.ensemble_config.ensemble_method,
            "num_models": len(self.models),
            "model_names": self.model_names,
            "require_all_models": self.ensemble_config.require_all_models,
            "fallback_model": self.ensemble_config.fallback_model
        })
        
        # Add individual model info
        model_info = []
        for i, model in enumerate(self.models):
            model_info.append({
                "name": self.model_names[i],
                "type": model.config.model_type.value,
                "is_trained": model.is_trained
            })
        info["models"] = model_info
        
        return info



# ./textclassify/ensemble/routing.py
"""Class routing ensemble for using different models for different classes."""

from typing import Dict, List, Union, Optional
import numpy as np

from ..core.types import ClassificationResult, ClassificationType
from ..core.exceptions import EnsembleError
from .base import BaseEnsemble


class ClassRoutingEnsemble(BaseEnsemble):
    """Ensemble that routes different classes to different models."""
    
    def __init__(self, ensemble_config):
        """Initialize class routing ensemble.
        
        Args:
            ensemble_config: Configuration for the ensemble
        """
        super().__init__(ensemble_config)
        
        # Set up routing rules
        self.routing_rules = ensemble_config.routing_rules or {}
        self.default_model_idx = 0  # Default to first model
        self.class_to_model = {}  # Maps class names to model indices
        
        # Fallback strategy
        self.fallback_strategy = "voting"  # "voting", "weighted", "first"
    
    def fit(self, training_data):
        """Fit all models and set up routing rules.
        
        Args:
            training_data: Training data for all models
        """
        super().fit(training_data)
        
        # Set up class-to-model mapping
        self._setup_routing()
    
    def _setup_routing(self):
        """Set up routing rules mapping classes to models."""
        # Clear existing mapping
        self.class_to_model = {}
        
        # Apply explicit routing rules
        for class_name, model_name in self.routing_rules.items():
            if model_name in self.model_names:
                model_idx = self.model_names.index(model_name)
                self.class_to_model[class_name] = model_idx
            else:
                print(f"Warning: Model '{model_name}' not found for class '{class_name}'")
        
        # For classes without explicit rules, use default model
        for class_name in self.classes_:
            if class_name not in self.class_to_model:
                self.class_to_model[class_name] = self.default_model_idx
    
    def add_routing_rule(self, class_name: str, model_name: str):
        """Add a routing rule for a specific class.
        
        Args:
            class_name: Name of the class
            model_name: Name of the model to use for this class
        """
        if model_name not in self.model_names:
            raise EnsembleError(f"Model '{model_name}' not found in ensemble", self.ensemble_config.ensemble_method)
        
        model_idx = self.model_names.index(model_name)
        self.class_to_model[class_name] = model_idx
        self.routing_rules[class_name] = model_name
    
    def _combine_predictions(self, model_results: List[ClassificationResult], texts: List[str]) -> List[Union[str, List[str]]]:
        """Combine predictions using class routing.
        
        Args:
            model_results: Results from all models
            texts: Original texts
            
        Returns:
            Combined predictions
        """
        if not model_results:
            return []
        
        num_texts = len(texts)
        combined_predictions = []
        
        for text_idx in range(num_texts):
            if self.classification_type == ClassificationType.MULTI_CLASS:
                # For multi-class, we need to determine which model to trust
                # Get predictions from all models first
                text_predictions = []
                for result in model_results:
                    if text_idx < len(result.predictions) and result.predictions[text_idx]:
                        text_predictions.append(result.predictions[text_idx])
                
                if text_predictions:
                    # Use routing rules to select the best prediction
                    best_prediction = self._route_multiclass_prediction(text_predictions)
                    combined_predictions.append(best_prediction)
                else:
                    combined_predictions.append(self.classes_[0] if self.classes_ else "unknown")
            
            else:
                # Multi-label classification
                final_labels = []
                
                # For each possible class, check if any model assigned to it predicts it
                for class_name in self.classes_:
                    model_idx = self.class_to_model.get(class_name, self.default_model_idx)
                    
                    if model_idx < len(model_results):
                        result = model_results[model_idx]
                        if text_idx < len(result.predictions) and result.predictions[text_idx]:
                            predicted_labels = result.predictions[text_idx]
                            if isinstance(predicted_labels, list) and class_name in predicted_labels:
                                final_labels.append(class_name)
                
                combined_predictions.append(final_labels)
        
        return combined_predictions
    
    def _combine_predictions_with_probabilities(
        self, 
        model_results: List[ClassificationResult], 
        texts: List[str]
    ) -> tuple:
        """Combine predictions and probabilities using class routing.
        
        Args:
            model_results: Results from all models
            texts: Original texts
            
        Returns:
            Tuple of (predictions, probabilities, confidence_scores)
        """
        if not model_results:
            return [], [], []
        
        num_texts = len(texts)
        combined_predictions = []
        combined_probabilities = []
        combined_confidence = []
        
        for text_idx in range(num_texts):
            if self.classification_type == ClassificationType.MULTI_CLASS:
                # Route each class to its designated model
                routed_probabilities = {}
                max_confidence = 0.0
                best_prediction = None
                
                for class_name in self.classes_:
                    model_idx = self.class_to_model.get(class_name, self.default_model_idx)
                    
                    if model_idx < len(model_results):
                        result = model_results[model_idx]
                        if (text_idx < len(result.predictions) and 
                            result.probabilities and 
                            text_idx < len(result.probabilities)):
                            
                            prob_dict = result.probabilities[text_idx]
                            if class_name in prob_dict:
                                prob = prob_dict[class_name]
                                routed_probabilities[class_name] = prob
                                
                                if prob > max_confidence:
                                    max_confidence = prob
                                    best_prediction = class_name
                            else:
                                routed_probabilities[class_name] = 0.0
                        else:
                            routed_probabilities[class_name] = 0.0
                    else:
                        routed_probabilities[class_name] = 0.0
                
                # Normalize probabilities to sum to 1
                total_prob = sum(routed_probabilities.values())
                if total_prob > 0:
                    for class_name in routed_probabilities:
                        routed_probabilities[class_name] /= total_prob
                else:
                    # Uniform distribution as fallback
                    uniform_prob = 1.0 / len(self.classes_) if self.classes_ else 0.0
                    routed_probabilities = {cls: uniform_prob for cls in self.classes_}
                    best_prediction = self.classes_[0] if self.classes_ else "unknown"
                    max_confidence = uniform_prob
                
                if best_prediction is None:
                    best_prediction = max(routed_probabilities, key=routed_probabilities.get)
                    max_confidence = routed_probabilities[best_prediction]
                
                combined_predictions.append(best_prediction)
                combined_probabilities.append(routed_probabilities)
                combined_confidence.append(max_confidence)
            
            else:
                # Multi-label classification
                routed_probabilities = {}
                final_labels = []
                max_confidence = 0.0
                
                for class_name in self.classes_:
                    model_idx = self.class_to_model.get(class_name, self.default_model_idx)
                    
                    if model_idx < len(model_results):
                        result = model_results[model_idx]
                        if (text_idx < len(result.predictions) and 
                            result.probabilities and 
                            text_idx < len(result.probabilities)):
                            
                            prob_dict = result.probabilities[text_idx]
                            if class_name in prob_dict:
                                prob = prob_dict[class_name]
                                routed_probabilities[class_name] = prob
                                
                                # Apply threshold
                                threshold = 0.5
                                if prob >= threshold:
                                    final_labels.append(class_name)
                                
                                max_confidence = max(max_confidence, prob)
                            else:
                                routed_probabilities[class_name] = 0.0
                        else:
                            routed_probabilities[class_name] = 0.0
                    else:
                        routed_probabilities[class_name] = 0.0
                
                combined_predictions.append(final_labels)
                combined_probabilities.append(routed_probabilities)
                combined_confidence.append(max_confidence)
        
        return combined_predictions, combined_probabilities, combined_confidence
    
    def _route_multiclass_prediction(self, predictions: List[str]) -> str:
        """Route multi-class prediction based on class-specific models.
        
        Args:
            predictions: List of predictions from all models
            
        Returns:
            Best prediction based on routing rules
        """
        # Count votes for each prediction, weighted by routing preference
        prediction_scores = {}
        
        for i, prediction in enumerate(predictions):
            if prediction in self.class_to_model:
                # This prediction comes from the model assigned to this class
                preferred_model_idx = self.class_to_model[prediction]
                if i == preferred_model_idx:
                    # This prediction comes from its preferred model
                    score = 2.0  # Higher weight for preferred model
                else:
                    score = 1.0  # Normal weight for non-preferred model
            else:
                score = 1.0  # Normal weight for unknown class
            
            if prediction not in prediction_scores:
                prediction_scores[prediction] = 0
            prediction_scores[prediction] += score
        
        if prediction_scores:
            return max(prediction_scores, key=prediction_scores.get)
        else:
            return self.classes_[0] if self.classes_ else "unknown"
    
    def get_model_for_class(self, class_name: str) -> Optional[int]:
        """Get the model index assigned to a specific class.
        
        Args:
            class_name: Name of the class
            
        Returns:
            Model index or None if class not found
        """
        return self.class_to_model.get(class_name)
    
    def get_routing_summary(self) -> Dict[str, str]:
        """Get a summary of routing rules.
        
        Returns:
            Dictionary mapping class names to model names
        """
        summary = {}
        for class_name, model_idx in self.class_to_model.items():
            if model_idx < len(self.model_names):
                summary[class_name] = self.model_names[model_idx]
            else:
                summary[class_name] = f"model_{model_idx}"
        return summary
    
    @property
    def model_info(self) -> Dict:
        """Get class routing ensemble information."""
        info = super().model_info
        info.update({
            "routing_rules": self.routing_rules,
            "class_to_model": self.class_to_model,
            "default_model_idx": self.default_model_idx,
            "fallback_strategy": self.fallback_strategy,
            "routing_summary": self.get_routing_summary()
        })
        return info



# ./textclassify/ensemble/voting.py
"""Voting ensemble for combining multiple classifiers."""

from collections import Counter
from typing import Dict, List, Union
import numpy as np

from ..core.types import ClassificationResult, ClassificationType
from .base import BaseEnsemble


class VotingEnsemble(BaseEnsemble):
    """Ensemble that combines predictions through voting."""
    
    def __init__(self, ensemble_config):
        """Initialize voting ensemble.
        
        Args:
            ensemble_config: Configuration for the ensemble
        """
        super().__init__(ensemble_config)
        self.voting_strategy = ensemble_config.ensemble_method
        
        # Validate voting strategy
        valid_strategies = ['hard', 'soft', 'majority', 'plurality']
        if self.voting_strategy not in valid_strategies:
            self.voting_strategy = 'majority'  # Default strategy
    
    def _combine_predictions(self, model_results: List[ClassificationResult], texts: List[str]) -> List[Union[str, List[str]]]:
        """Combine predictions using voting.
        
        Args:
            model_results: Results from all models
            texts: Original texts
            
        Returns:
            Combined predictions
        """
        if not model_results:
            return []
        
        num_texts = len(texts)
        combined_predictions = []
        
        for text_idx in range(num_texts):
            if self.classification_type == ClassificationType.MULTI_CLASS:
                # Collect predictions for this text from all models
                text_predictions = []
                for result in model_results:
                    if text_idx < len(result.predictions) and result.predictions[text_idx]:
                        text_predictions.append(result.predictions[text_idx])
                
                # Vote for the best prediction
                if text_predictions:
                    if self.voting_strategy in ['majority', 'plurality']:
                        # Use most common prediction
                        vote_counts = Counter(text_predictions)
                        most_common = vote_counts.most_common(1)[0]
                        
                        if self.voting_strategy == 'majority' and most_common[1] <= len(text_predictions) // 2:
                            # No majority, use first prediction as fallback
                            combined_prediction = text_predictions[0]
                        else:
                            combined_prediction = most_common[0]
                    else:
                        # Default to first prediction
                        combined_prediction = text_predictions[0]
                else:
                    # No valid predictions, use first class as fallback
                    combined_prediction = self.classes_[0] if self.classes_ else "unknown"
                
                combined_predictions.append(combined_prediction)
            
            else:
                # Multi-label classification
                # Collect all predicted labels from all models
                all_labels = set()
                label_votes = Counter()
                
                for result in model_results:
                    if text_idx < len(result.predictions) and result.predictions[text_idx]:
                        labels = result.predictions[text_idx]
                        if isinstance(labels, list):
                            for label in labels:
                                label_votes[label] += 1
                                all_labels.add(label)
                
                # Determine final labels based on voting strategy
                if self.voting_strategy == 'majority':
                    # Require majority vote for each label
                    threshold = len(model_results) // 2 + 1
                    final_labels = [label for label, count in label_votes.items() if count >= threshold]
                elif self.voting_strategy == 'plurality':
                    # Use any label that got at least one vote
                    final_labels = list(label_votes.keys())
                else:
                    # Default: use labels that got more than half the votes
                    threshold = max(1, len(model_results) // 2)
                    final_labels = [label for label, count in label_votes.items() if count > threshold]
                
                combined_predictions.append(final_labels)
        
        return combined_predictions
    
    def _combine_predictions_with_probabilities(
        self, 
        model_results: List[ClassificationResult], 
        texts: List[str]
    ) -> tuple:
        """Combine predictions and probabilities using voting.
        
        Args:
            model_results: Results from all models
            texts: Original texts
            
        Returns:
            Tuple of (predictions, probabilities, confidence_scores)
        """
        if not model_results:
            return [], [], []
        
        num_texts = len(texts)
        combined_predictions = []
        combined_probabilities = []
        combined_confidence = []
        
        for text_idx in range(num_texts):
            if self.classification_type == ClassificationType.MULTI_CLASS:
                # Collect probabilities for this text from all models
                text_probabilities = []
                valid_models = 0
                
                for result in model_results:
                    if (text_idx < len(result.predictions) and 
                        result.probabilities and 
                        text_idx < len(result.probabilities)):
                        text_probabilities.append(result.probabilities[text_idx])
                        valid_models += 1
                
                if text_probabilities and valid_models > 0:
                    # Average probabilities across models
                    avg_probabilities = {}
                    for class_name in self.classes_:
                        class_probs = []
                        for prob_dict in text_probabilities:
                            if class_name in prob_dict:
                                class_probs.append(prob_dict[class_name])
                        
                        if class_probs:
                            avg_probabilities[class_name] = np.mean(class_probs)
                        else:
                            avg_probabilities[class_name] = 0.0
                    
                    # Get prediction with highest average probability
                    best_class = max(avg_probabilities, key=avg_probabilities.get)
                    confidence = avg_probabilities[best_class]
                    
                    combined_predictions.append(best_class)
                    combined_probabilities.append(avg_probabilities)
                    combined_confidence.append(confidence)
                
                else:
                    # Fallback to hard voting
                    hard_predictions = self._combine_predictions(model_results, [texts[text_idx]])
                    prediction = hard_predictions[0] if hard_predictions else (self.classes_[0] if self.classes_ else "unknown")
                    
                    # Create uniform probabilities
                    uniform_prob = 1.0 / len(self.classes_) if self.classes_ else 0.0
                    probabilities = {cls: uniform_prob for cls in self.classes_}
                    
                    combined_predictions.append(prediction)
                    combined_probabilities.append(probabilities)
                    combined_confidence.append(uniform_prob)
            
            else:
                # Multi-label classification
                # Collect probabilities for each class
                class_probabilities = {cls: [] for cls in self.classes_}
                valid_models = 0
                
                for result in model_results:
                    if (text_idx < len(result.predictions) and 
                        result.probabilities and 
                        text_idx < len(result.probabilities)):
                        prob_dict = result.probabilities[text_idx]
                        for class_name in self.classes_:
                            if class_name in prob_dict:
                                class_probabilities[class_name].append(prob_dict[class_name])
                        valid_models += 1
                
                if valid_models > 0:
                    # Average probabilities for each class
                    avg_probabilities = {}
                    for class_name in self.classes_:
                        if class_probabilities[class_name]:
                            avg_probabilities[class_name] = np.mean(class_probabilities[class_name])
                        else:
                            avg_probabilities[class_name] = 0.0
                    
                    # Apply threshold to determine final labels
                    threshold = 0.5  # Default threshold
                    if hasattr(self.ensemble_config, 'threshold'):
                        threshold = self.ensemble_config.threshold
                    
                    final_labels = [cls for cls, prob in avg_probabilities.items() if prob >= threshold]
                    confidence = max(avg_probabilities.values()) if avg_probabilities else 0.0
                    
                    combined_predictions.append(final_labels)
                    combined_probabilities.append(avg_probabilities)
                    combined_confidence.append(confidence)
                
                else:
                    # Fallback to hard voting
                    hard_predictions = self._combine_predictions(model_results, [texts[text_idx]])
                    prediction = hard_predictions[0] if hard_predictions else []
                    
                    # Create uniform probabilities
                    uniform_prob = 0.5
                    probabilities = {cls: uniform_prob for cls in self.classes_}
                    
                    combined_predictions.append(prediction)
                    combined_probabilities.append(probabilities)
                    combined_confidence.append(uniform_prob)
        
        return combined_predictions, combined_probabilities, combined_confidence



# ./textclassify/ensemble/weighted.py
"""Weighted ensemble for combining multiple classifiers."""

from typing import Dict, List, Union
import numpy as np

from ..core.types import ClassificationResult, ClassificationType
from ..core.exceptions import EnsembleError
from .base import BaseEnsemble


class WeightedEnsemble(BaseEnsemble):
    """Ensemble that combines predictions using weighted averages."""
    
    def __init__(self, ensemble_config):
        """Initialize weighted ensemble.
        
        Args:
            ensemble_config: Configuration for the ensemble
        """
        super().__init__(ensemble_config)
        
        # Set weights
        if ensemble_config.weights:
            if len(ensemble_config.weights) != len(ensemble_config.models):
                raise EnsembleError(
                    "Number of weights must match number of models",
                    ensemble_config.ensemble_method
                )
            self.weights = np.array(ensemble_config.weights)
        else:
            # Equal weights by default
            self.weights = np.ones(len(ensemble_config.models))
        
        # Normalize weights
        self.weights = self.weights / np.sum(self.weights)
    
    def add_model(self, model, name=None, weight=None):
        """Add a model to the ensemble with optional weight.
        
        Args:
            model: Classifier to add
            name: Optional name for the model
            weight: Optional weight for the model
        """
        super().add_model(model, name)
        
        if weight is not None:
            # Update weights array
            if len(self.weights) == len(self.models) - 1:
                # First time adding weight, extend array
                self.weights = np.append(self.weights, weight)
            else:
                # Update existing weight
                self.weights[-1] = weight
            
            # Renormalize weights
            self.weights = self.weights / np.sum(self.weights)
    
    def _combine_predictions(self, model_results: List[ClassificationResult], texts: List[str]) -> List[Union[str, List[str]]]:
        """Combine predictions using weights.
        
        Args:
            model_results: Results from all models
            texts: Original texts
            
        Returns:
            Combined predictions
        """
        if not model_results:
            return []
        
        num_texts = len(texts)
        combined_predictions = []
        
        for text_idx in range(num_texts):
            if self.classification_type == ClassificationType.MULTI_CLASS:
                # For multi-class without probabilities, use weighted voting
                class_votes = {}
                total_weight = 0
                
                for model_idx, result in enumerate(model_results):
                    if text_idx < len(result.predictions) and result.predictions[text_idx]:
                        prediction = result.predictions[text_idx]
                        weight = self.weights[model_idx] if model_idx < len(self.weights) else 1.0
                        
                        if prediction not in class_votes:
                            class_votes[prediction] = 0
                        class_votes[prediction] += weight
                        total_weight += weight
                
                if class_votes:
                    # Choose class with highest weighted vote
                    best_class = max(class_votes, key=class_votes.get)
                    combined_predictions.append(best_class)
                else:
                    # Fallback
                    combined_predictions.append(self.classes_[0] if self.classes_ else "unknown")
            
            else:
                # Multi-label classification
                label_scores = {}
                total_weight = 0
                
                for model_idx, result in enumerate(model_results):
                    if text_idx < len(result.predictions) and result.predictions[text_idx]:
                        labels = result.predictions[text_idx]
                        weight = self.weights[model_idx] if model_idx < len(self.weights) else 1.0
                        
                        if isinstance(labels, list):
                            for label in labels:
                                if label not in label_scores:
                                    label_scores[label] = 0
                                label_scores[label] += weight
                        
                        total_weight += weight
                
                if label_scores and total_weight > 0:
                    # Normalize scores and apply threshold
                    threshold = 0.5  # Default threshold
                    final_labels = []
                    
                    for label, score in label_scores.items():
                        normalized_score = score / total_weight
                        if normalized_score >= threshold:
                            final_labels.append(label)
                    
                    combined_predictions.append(final_labels)
                else:
                    combined_predictions.append([])
        
        return combined_predictions
    
    def _combine_predictions_with_probabilities(
        self, 
        model_results: List[ClassificationResult], 
        texts: List[str]
    ) -> tuple:
        """Combine predictions and probabilities using weights.
        
        Args:
            model_results: Results from all models
            texts: Original texts
            
        Returns:
            Tuple of (predictions, probabilities, confidence_scores)
        """
        if not model_results:
            return [], [], []
        
        num_texts = len(texts)
        combined_predictions = []
        combined_probabilities = []
        combined_confidence = []
        
        for text_idx in range(num_texts):
            if self.classification_type == ClassificationType.MULTI_CLASS:
                # Weighted average of probabilities
                weighted_probabilities = {}
                total_weight = 0
                
                for model_idx, result in enumerate(model_results):
                    if (text_idx < len(result.predictions) and 
                        result.probabilities and 
                        text_idx < len(result.probabilities)):
                        
                        prob_dict = result.probabilities[text_idx]
                        weight = self.weights[model_idx] if model_idx < len(self.weights) else 1.0
                        
                        for class_name, prob in prob_dict.items():
                            if class_name not in weighted_probabilities:
                                weighted_probabilities[class_name] = 0
                            weighted_probabilities[class_name] += prob * weight
                        
                        total_weight += weight
                
                if weighted_probabilities and total_weight > 0:
                    # Normalize probabilities
                    for class_name in weighted_probabilities:
                        weighted_probabilities[class_name] /= total_weight
                    
                    # Ensure all classes are represented
                    for class_name in self.classes_:
                        if class_name not in weighted_probabilities:
                            weighted_probabilities[class_name] = 0.0
                    
                    # Get prediction with highest probability
                    best_class = max(weighted_probabilities, key=weighted_probabilities.get)
                    confidence = weighted_probabilities[best_class]
                    
                    combined_predictions.append(best_class)
                    combined_probabilities.append(weighted_probabilities)
                    combined_confidence.append(confidence)
                
                else:
                    # Fallback to hard voting
                    hard_predictions = self._combine_predictions(model_results, [texts[text_idx]])
                    prediction = hard_predictions[0] if hard_predictions else (self.classes_[0] if self.classes_ else "unknown")
                    
                    # Create uniform probabilities
                    uniform_prob = 1.0 / len(self.classes_) if self.classes_ else 0.0
                    probabilities = {cls: uniform_prob for cls in self.classes_}
                    
                    combined_predictions.append(prediction)
                    combined_probabilities.append(probabilities)
                    combined_confidence.append(uniform_prob)
            
            else:
                # Multi-label classification
                weighted_probabilities = {}
                total_weight = 0
                
                for model_idx, result in enumerate(model_results):
                    if (text_idx < len(result.predictions) and 
                        result.probabilities and 
                        text_idx < len(result.probabilities)):
                        
                        prob_dict = result.probabilities[text_idx]
                        weight = self.weights[model_idx] if model_idx < len(self.weights) else 1.0
                        
                        for class_name, prob in prob_dict.items():
                            if class_name not in weighted_probabilities:
                                weighted_probabilities[class_name] = 0
                            weighted_probabilities[class_name] += prob * weight
                        
                        total_weight += weight
                
                if weighted_probabilities and total_weight > 0:
                    # Normalize probabilities
                    for class_name in weighted_probabilities:
                        weighted_probabilities[class_name] /= total_weight
                    
                    # Ensure all classes are represented
                    for class_name in self.classes_:
                        if class_name not in weighted_probabilities:
                            weighted_probabilities[class_name] = 0.0
                    
                    # Apply threshold to determine final labels
                    threshold = 0.5  # Default threshold
                    final_labels = [cls for cls, prob in weighted_probabilities.items() if prob >= threshold]
                    confidence = max(weighted_probabilities.values()) if weighted_probabilities else 0.0
                    
                    combined_predictions.append(final_labels)
                    combined_probabilities.append(weighted_probabilities)
                    combined_confidence.append(confidence)
                
                else:
                    # Fallback to hard voting
                    hard_predictions = self._combine_predictions(model_results, [texts[text_idx]])
                    prediction = hard_predictions[0] if hard_predictions else []
                    
                    # Create uniform probabilities
                    uniform_prob = 0.5
                    probabilities = {cls: uniform_prob for cls in self.classes_}
                    
                    combined_predictions.append(prediction)
                    combined_probabilities.append(probabilities)
                    combined_confidence.append(uniform_prob)
        
        return combined_predictions, combined_probabilities, combined_confidence
    
    def update_weights(self, new_weights: List[float]) -> None:
        """Update model weights.
        
        Args:
            new_weights: New weights for the models
        """
        if len(new_weights) != len(self.models):
            raise EnsembleError(
                "Number of weights must match number of models",
                self.ensemble_config.ensemble_method
            )
        
        self.weights = np.array(new_weights)
        self.weights = self.weights / np.sum(self.weights)  # Normalize
    
    @property
    def model_info(self) -> Dict:
        """Get weighted ensemble information."""
        info = super().model_info
        info.update({
            "weights": self.weights.tolist(),
            "normalized_weights": True
        })
        return info



# ./textclassify/examples/__init__.py
"""Examples and tutorials for textclassify package."""

# This module contains example scripts demonstrating various features
# of the textclassify package



# ./textclassify/examples/ensemble_example.py
"""
Advanced Ensemble Methods Example

This example demonstrates advanced ensemble strategies for combining
multiple classifiers to optimize performance.
"""

import os
import numpy as np
from textclassify import (
    OpenAIClassifier, ClaudeClassifier, GeminiClassifier, DeepSeekClassifier,
    RoBERTaClassifier, VotingEnsemble, WeightedEnsemble, ClassRoutingEnsemble,
    Config, ClassificationType, ModelType
)
from textclassify.utils import evaluate_predictions, compare_models, split_data
from textclassify.core.types import TrainingData, ModelConfig, EnsembleConfig


def create_comprehensive_dataset():
    """Create a more comprehensive dataset for ensemble testing."""
    texts = [
        # Technology
        "The new smartphone features advanced AI capabilities and 5G connectivity.",
        "Machine learning algorithms are revolutionizing data analysis.",
        "Cloud computing provides scalable infrastructure for businesses.",
        
        # Sports
        "The basketball team won the championship with an amazing final shot.",
        "The soccer match ended in a thrilling penalty shootout.",
        "Olympic athletes train for years to compete at the highest level.",
        
        # Entertainment
        "The movie premiere was attended by many Hollywood celebrities.",
        "The concert featured incredible live performances and stage effects.",
        "The TV series finale broke viewership records.",
        
        # Business
        "The company reported record profits in the quarterly earnings call.",
        "Stock markets showed volatility due to economic uncertainty.",
        "The startup secured significant funding from venture capitalists.",
        
        # Health
        "Regular exercise and healthy diet are essential for wellbeing.",
        "Medical research has led to breakthrough treatments for diseases.",
        "Mental health awareness is becoming increasingly important.",
        
        # Education
        "Online learning platforms have transformed modern education.",
        "University students are adapting to hybrid learning models.",
        "Educational technology is making learning more accessible."
    ]
    
    labels = [
        "technology", "technology", "technology",
        "sports", "sports", "sports",
        "entertainment", "entertainment", "entertainment",
        "business", "business", "business",
        "health", "health", "health",
        "education", "education", "education"
    ]
    
    return TrainingData(
        texts=texts,
        labels=labels,
        classification_type=ClassificationType.MULTI_CLASS
    )


def create_individual_classifiers():
    """Create individual classifiers for ensemble."""
    classifiers = {}
    
    # LLM Classifiers
    llm_configs = {
        "openai": {
            "model_name": "gpt-3.5-turbo",
            "api_key": os.getenv("OPENAI_API_KEY"),
            "classifier_class": OpenAIClassifier
        },
        "claude": {
            "model_name": "claude-3-haiku-20240307",
            "api_key": os.getenv("CLAUDE_API_KEY"),
            "classifier_class": ClaudeClassifier
        },
        "gemini": {
            "model_name": "gemini-1.5-flash",
            "api_key": os.getenv("GEMINI_API_KEY"),
            "classifier_class": GeminiClassifier
        },
        "deepseek": {
            "model_name": "deepseek-chat",
            "api_key": os.getenv("DEEPSEEK_API_KEY"),
            "classifier_class": DeepSeekClassifier
        }
    }
    
    for name, config_info in llm_configs.items():
        if config_info["api_key"]:
            config = ModelConfig(
                model_name=config_info["model_name"],
                model_type=ModelType.LLM,
                api_key=config_info["api_key"],
                parameters={
                    "temperature": 0.1,
                    "max_tokens": 50,
                    "max_examples": 3
                }
            )
            classifiers[name] = config_info["classifier_class"](config)
    
    # RoBERTa Classifier (if transformers available)
    try:
        roberta_config = ModelConfig(
            model_name="roberta-base",
            model_type=ModelType.TRADITIONAL_ML,
            parameters={
                "max_length": 128,
                "batch_size": 8,
                "num_epochs": 2,
                "learning_rate": 2e-5
            }
        )
        classifiers["roberta"] = RoBERTaClassifier(roberta_config)
    except ImportError:
        print("RoBERTa not available (transformers not installed)")
    
    return classifiers


def example_model_comparison():
    """Compare individual models before ensembling."""
    print("=== Individual Model Comparison ===")
    
    # Create dataset
    training_data = create_comprehensive_dataset()
    train_data, test_data = split_data(training_data, train_ratio=0.7)
    
    print(f"Training samples: {len(train_data.texts)}")
    print(f"Test samples: {len(test_data.texts)}")
    print()
    
    # Create classifiers
    classifiers = create_individual_classifiers()
    
    if not classifiers:
        print("No classifiers available (missing API keys or dependencies)")
        return None, None, None
    
    print(f"Available classifiers: {list(classifiers.keys())}")
    
    # Train and evaluate each classifier
    results = []
    model_names = []
    
    for name, classifier in classifiers.items():
        print(f"\nTraining {name}...")
        try:
            classifier.fit(train_data)
            result = classifier.predict(test_data.texts)
            results.append(result)
            model_names.append(name)
            
            # Quick evaluation
            metrics = evaluate_predictions(result, test_data.labels)
            print(f"{name} accuracy: {metrics['accuracy']:.3f}")
            
        except Exception as e:
            print(f"Error with {name}: {str(e)}")
    
    # Compare all models
    if len(results) > 1:
        comparison = compare_models(results, test_data.labels, model_names)
        
        print("\n=== Model Comparison Summary ===")
        for metric in ['accuracy', 'macro_f1', 'weighted_f1']:
            print(f"\n{metric.upper()}:")
            for model, score in comparison['summary'][metric].items():
                print(f"  {model}: {score:.3f}")
            print(f"  Best: {comparison['best_models'][metric]}")
    
    return classifiers, train_data, test_data


def example_voting_ensemble(classifiers, train_data, test_data):
    """Demonstrate voting ensemble with different strategies."""
    print("\n=== Voting Ensemble Strategies ===")
    
    if len(classifiers) < 2:
        print("Need at least 2 classifiers for ensemble")
        return
    
    # Test different voting strategies
    voting_strategies = ['majority', 'plurality']
    
    for strategy in voting_strategies:
        print(f"\n--- {strategy.upper()} Voting ---")
        
        # Create ensemble config
        model_configs = []
        for name, classifier in classifiers.items():
            model_configs.append(classifier.config)
        
        ensemble_config = EnsembleConfig(
            models=model_configs,
            ensemble_method=strategy
        )
        
        # Create ensemble
        ensemble = VotingEnsemble(ensemble_config)
        
        # Add classifiers
        for name, classifier in classifiers.items():
            ensemble.add_model(classifier, name)
        
        # Train and test
        ensemble.fit(train_data)
        result = ensemble.predict(test_data.texts)
        
        # Evaluate
        metrics = evaluate_predictions(result, test_data.labels)
        print(f"Accuracy: {metrics['accuracy']:.3f}")
        print(f"Macro F1: {metrics['macro_f1']:.3f}")


def example_weighted_ensemble(classifiers, train_data, test_data):
    """Demonstrate weighted ensemble with performance-based weights."""
    print("\n=== Weighted Ensemble ===")
    
    if len(classifiers) < 2:
        print("Need at least 2 classifiers for ensemble")
        return
    
    # Calculate individual model performance to determine weights
    model_performances = {}
    
    for name, classifier in classifiers.items():
        # Use a small validation set to calculate weights
        val_result = classifier.predict(test_data.texts[:5])  # Small sample
        val_metrics = evaluate_predictions(val_result, test_data.labels[:5])
        model_performances[name] = val_metrics['accuracy']
    
    print("Individual model performances:")
    for name, perf in model_performances.items():
        print(f"  {name}: {perf:.3f}")
    
    # Calculate weights based on performance
    total_perf = sum(model_performances.values())
    weights = [model_performances[name] / total_perf for name in classifiers.keys()]
    
    print(f"Calculated weights: {[f'{w:.3f}' for w in weights]}")
    
    # Create weighted ensemble
    model_configs = [classifier.config for classifier in classifiers.values()]
    ensemble_config = EnsembleConfig(
        models=model_configs,
        ensemble_method="weighted",
        weights=weights
    )
    
    ensemble = WeightedEnsemble(ensemble_config)
    
    # Add classifiers with weights
    for (name, classifier), weight in zip(classifiers.items(), weights):
        ensemble.add_model(classifier, name, weight)
    
    # Train and test
    ensemble.fit(train_data)
    result = ensemble.predict_proba(test_data.texts)
    
    # Evaluate
    metrics = evaluate_predictions(result, test_data.labels)
    print(f"\nWeighted ensemble performance:")
    print(f"Accuracy: {metrics['accuracy']:.3f}")
    print(f"Macro F1: {metrics['macro_f1']:.3f}")
    
    # Show final weights
    print(f"Final weights: {[f'{w:.3f}' for w in ensemble.weights]}")


def example_class_routing(classifiers, train_data, test_data):
    """Demonstrate class-specific routing ensemble."""
    print("\n=== Class Routing Ensemble ===")
    
    if len(classifiers) < 2:
        print("Need at least 2 classifiers for routing")
        return
    
    # Define routing rules based on domain expertise
    # For example: technical topics to one model, creative topics to another
    classifier_names = list(classifiers.keys())
    
    routing_rules = {
        "technology": classifier_names[0],
        "business": classifier_names[0],
        "sports": classifier_names[1] if len(classifier_names) > 1 else classifier_names[0],
        "entertainment": classifier_names[1] if len(classifier_names) > 1 else classifier_names[0],
        "health": classifier_names[0],
        "education": classifier_names[0]
    }
    
    print("Routing rules:")
    for class_name, model_name in routing_rules.items():
        print(f"  {class_name} -> {model_name}")
    
    # Create routing ensemble
    model_configs = [classifier.config for classifier in classifiers.values()]
    ensemble_config = EnsembleConfig(
        models=model_configs,
        ensemble_method="routing",
        routing_rules=routing_rules
    )
    
    ensemble = ClassRoutingEnsemble(ensemble_config)
    
    # Add classifiers
    for name, classifier in classifiers.items():
        ensemble.add_model(classifier, name)
    
    # Train and test
    ensemble.fit(train_data)
    result = ensemble.predict(test_data.texts)
    
    # Evaluate
    metrics = evaluate_predictions(result, test_data.labels)
    print(f"\nClass routing performance:")
    print(f"Accuracy: {metrics['accuracy']:.3f}")
    print(f"Macro F1: {metrics['macro_f1']:.3f}")
    
    # Show which model was used for each prediction
    print(f"\nRouting summary:")
    routing_summary = ensemble.get_routing_summary()
    for class_name, model_name in routing_summary.items():
        print(f"  {class_name} -> {model_name}")


def example_ensemble_comparison(classifiers, train_data, test_data):
    """Compare different ensemble methods."""
    print("\n=== Ensemble Method Comparison ===")
    
    if len(classifiers) < 2:
        print("Need at least 2 classifiers for ensemble comparison")
        return
    
    ensemble_results = []
    ensemble_names = []
    
    # Voting ensemble
    try:
        model_configs = [classifier.config for classifier in classifiers.values()]
        voting_config = EnsembleConfig(models=model_configs, ensemble_method="majority")
        voting_ensemble = VotingEnsemble(voting_config)
        
        for name, classifier in classifiers.items():
            voting_ensemble.add_model(classifier, name)
        
        voting_ensemble.fit(train_data)
        voting_result = voting_ensemble.predict(test_data.texts)
        ensemble_results.append(voting_result)
        ensemble_names.append("voting")
    except Exception as e:
        print(f"Voting ensemble error: {e}")
    
    # Weighted ensemble
    try:
        weights = [1.0 / len(classifiers)] * len(classifiers)  # Equal weights
        weighted_config = EnsembleConfig(
            models=model_configs, 
            ensemble_method="weighted", 
            weights=weights
        )
        weighted_ensemble = WeightedEnsemble(weighted_config)
        
        for name, classifier in classifiers.items():
            weighted_ensemble.add_model(classifier, name)
        
        weighted_ensemble.fit(train_data)
        weighted_result = weighted_ensemble.predict(test_data.texts)
        ensemble_results.append(weighted_result)
        ensemble_names.append("weighted")
    except Exception as e:
        print(f"Weighted ensemble error: {e}")
    
    # Compare ensemble methods
    if len(ensemble_results) > 1:
        comparison = compare_models(ensemble_results, test_data.labels, ensemble_names)
        
        print("Ensemble comparison:")
        for metric in ['accuracy', 'macro_f1']:
            print(f"\n{metric.upper()}:")
            for method, score in comparison['summary'][metric].items():
                print(f"  {method}: {score:.3f}")
            print(f"  Best: {comparison['best_models'][metric]}")


def main():
    """Run all ensemble examples."""
    print("TextClassify Advanced Ensemble Examples")
    print("=" * 50)
    
    try:
        # Compare individual models
        classifiers, train_data, test_data = example_model_comparison()
        
        if not classifiers:
            print("No classifiers available. Please set API keys:")
            print("  export OPENAI_API_KEY='your-key-here'")
            print("  export CLAUDE_API_KEY='your-key-here'")
            print("  export GEMINI_API_KEY='your-key-here'")
            print("  export DEEPSEEK_API_KEY='your-key-here'")
            return
        
        # Ensemble examples
        example_voting_ensemble(classifiers, train_data, test_data)
        example_weighted_ensemble(classifiers, train_data, test_data)
        example_class_routing(classifiers, train_data, test_data)
        example_ensemble_comparison(classifiers, train_data, test_data)
        
        print("\n=== Summary ===")
        print("Ensemble methods can significantly improve classification performance")
        print("by combining the strengths of different models:")
        print("- Voting: Simple majority/plurality voting")
        print("- Weighted: Performance-based model weighting")
        print("- Routing: Class-specific model assignment")
        print("Choose the method that best fits your use case and data!")
    
    except Exception as e:
        print(f"Error running ensemble examples: {str(e)}")


if __name__ == "__main__":
    main()



# ./textclassify/examples/multi_class_example.py
"""
Multi-class Text Classification Example

This example demonstrates how to use the textclassify package for multi-class
text classification using different models and ensemble methods.
"""

import os
from textclassify import (
    OpenAIClassifier, ClaudeClassifier, RoBERTaClassifier,
    VotingEnsemble, WeightedEnsemble,
    Config, APIKeyManager,
    ClassificationType, ModelType
)
from textclassify.utils import DataLoader, split_data, evaluate_predictions
from textclassify.core.types import TrainingData, ModelConfig, EnsembleConfig


def create_sample_data():
    """Create sample data for demonstration."""
    texts = [
        "I love this movie! It's absolutely fantastic.",
        "This film is terrible. I want my money back.",
        "The movie was okay, nothing special.",
        "Amazing cinematography and great acting!",
        "Boring and predictable plot.",
        "One of the best movies I've ever seen!",
        "Not worth watching, very disappointing.",
        "Decent movie with some good moments.",
        "Incredible story and brilliant performances.",
        "Waste of time, poorly made film."
    ]
    
    labels = [
        "positive", "negative", "neutral",
        "positive", "negative", "positive",
        "negative", "neutral", "positive", "negative"
    ]
    
    return TrainingData(
        texts=texts,
        labels=labels,
        classification_type=ClassificationType.MULTI_CLASS
    )


def example_openai_classifier():
    """Example using OpenAI classifier."""
    print("=== OpenAI Classifier Example ===")
    
    # Create configuration
    config = ModelConfig(
        model_name="gpt-3.5-turbo",
        model_type=ModelType.LLM,
        api_key=os.getenv("OPENAI_API_KEY"),  # Set your API key
        parameters={
            "temperature": 0.1,
            "max_tokens": 50
        }
    )
    
    # Create classifier
    classifier = OpenAIClassifier(config)
    
    # Create sample data
    training_data = create_sample_data()
    
    # Train (for LLMs, this sets up few-shot examples)
    classifier.fit(training_data)
    
    # Test predictions
    test_texts = [
        "This movie is absolutely wonderful!",
        "I didn't like it at all.",
        "It was an average film."
    ]
    
    # Get predictions
    result = classifier.predict(test_texts)
    print(f"Predictions: {result.predictions}")
    
    # Get predictions with probabilities
    result_proba = classifier.predict_proba(test_texts)
    print(f"Predictions with probabilities:")
    for i, (text, pred, prob) in enumerate(zip(test_texts, result_proba.predictions, result_proba.probabilities)):
        print(f"  Text: {text}")
        print(f"  Prediction: {pred}")
        print(f"  Probabilities: {prob}")
        print()


def example_roberta_classifier():
    """Example using RoBERTa classifier."""
    print("=== RoBERTa Classifier Example ===")
    
    # Create configuration
    config = ModelConfig(
        model_name="roberta-base",
        model_type=ModelType.TRADITIONAL_ML,
        parameters={
            "max_length": 128,
            "batch_size": 8,
            "num_epochs": 2,
            "learning_rate": 2e-5
        }
    )
    
    # Create classifier
    classifier = RoBERTaClassifier(config)
    
    # Create sample data (you would use more data in practice)
    training_data = create_sample_data()
    
    # Split data
    train_data, val_data = split_data(training_data, train_ratio=0.8)
    
    print(f"Training samples: {len(train_data.texts)}")
    print(f"Validation samples: {len(val_data.texts)}")
    
    # Train the model
    print("Training RoBERTa model...")
    classifier.fit(train_data)
    
    # Test predictions
    result = classifier.predict(val_data.texts)
    print(f"Predictions: {result.predictions}")
    
    # Evaluate performance
    metrics = evaluate_predictions(result, val_data.labels)
    print(f"Accuracy: {metrics['accuracy']:.3f}")
    print(f"Macro F1: {metrics['macro_f1']:.3f}")


def example_ensemble_classifier():
    """Example using ensemble methods."""
    print("=== Ensemble Classifier Example ===")
    
    # Create individual model configurations
    openai_config = ModelConfig(
        model_name="gpt-3.5-turbo",
        model_type=ModelType.LLM,
        api_key=os.getenv("OPENAI_API_KEY"),
        parameters={"temperature": 0.1, "max_tokens": 50}
    )
    
    claude_config = ModelConfig(
        model_name="claude-3-haiku-20240307",
        model_type=ModelType.LLM,
        api_key=os.getenv("CLAUDE_API_KEY"),
        parameters={"temperature": 0.1, "max_tokens": 50}
    )
    
    # Create ensemble configuration
    ensemble_config = EnsembleConfig(
        models=[openai_config, claude_config],
        ensemble_method="voting"
    )
    
    # Create ensemble
    ensemble = VotingEnsemble(ensemble_config)
    
    # Add individual classifiers
    if openai_config.api_key:
        ensemble.add_model(OpenAIClassifier(openai_config), "openai")
    if claude_config.api_key:
        ensemble.add_model(ClaudeClassifier(claude_config), "claude")
    
    if len(ensemble.models) == 0:
        print("No API keys available for ensemble example")
        return
    
    # Train ensemble
    training_data = create_sample_data()
    ensemble.fit(training_data)
    
    # Test predictions
    test_texts = [
        "This movie is absolutely wonderful!",
        "I didn't like it at all."
    ]
    
    result = ensemble.predict(test_texts)
    print(f"Ensemble predictions: {result.predictions}")
    
    # Get model info
    print(f"Ensemble info: {ensemble.model_info}")


def example_weighted_ensemble():
    """Example using weighted ensemble."""
    print("=== Weighted Ensemble Example ===")
    
    # Create configurations with different weights
    model_configs = [
        ModelConfig(
            model_name="gpt-3.5-turbo",
            model_type=ModelType.LLM,
            api_key=os.getenv("OPENAI_API_KEY")
        ),
        ModelConfig(
            model_name="claude-3-haiku-20240307",
            model_type=ModelType.LLM,
            api_key=os.getenv("CLAUDE_API_KEY")
        )
    ]
    
    # Create weighted ensemble with custom weights
    ensemble_config = EnsembleConfig(
        models=model_configs,
        ensemble_method="weighted",
        weights=[0.7, 0.3]  # Give more weight to first model
    )
    
    ensemble = WeightedEnsemble(ensemble_config)
    
    # Add models with weights
    if model_configs[0].api_key:
        ensemble.add_model(OpenAIClassifier(model_configs[0]), "openai", weight=0.7)
    if model_configs[1].api_key:
        ensemble.add_model(ClaudeClassifier(model_configs[1]), "claude", weight=0.3)
    
    if len(ensemble.models) == 0:
        print("No API keys available for weighted ensemble example")
        return
    
    # Train and test
    training_data = create_sample_data()
    ensemble.fit(training_data)
    
    test_texts = ["This is an amazing movie!"]
    result = ensemble.predict_proba(test_texts)
    
    print(f"Weighted ensemble prediction: {result.predictions[0]}")
    print(f"Probabilities: {result.probabilities[0]}")
    print(f"Weights: {ensemble.weights}")


def example_configuration_management():
    """Example using configuration management."""
    print("=== Configuration Management Example ===")
    
    # Create and configure API key manager
    api_manager = APIKeyManager()
    
    # Check for existing API keys
    providers = ['openai', 'claude', 'gemini', 'deepseek']
    for provider in providers:
        has_key = api_manager.has_key(provider)
        print(f"{provider}: {'✓' if has_key else '✗'}")
    
    # Create configuration
    config = Config()
    
    # Set some configuration values
    config.set('llm.default_provider', 'openai')
    config.set('general.default_batch_size', 16)
    
    # Create model config from main config
    model_config = config.create_model_config(
        model_name="gpt-3.5-turbo",
        model_type=ModelType.LLM,
        provider="openai"
    )
    
    print(f"Model config: {model_config.model_name}")
    print(f"Batch size: {model_config.batch_size}")
    
    # Save configuration
    config_path = "example_config.yaml"
    config.save(config_path)
    print(f"Configuration saved to {config_path}")


def main():
    """Run all examples."""
    print("TextClassify Multi-class Classification Examples")
    print("=" * 50)
    
    try:
        # Configuration example
        example_configuration_management()
        print()
        
        # Individual classifier examples
        if os.getenv("OPENAI_API_KEY"):
            example_openai_classifier()
            print()
        else:
            print("Skipping OpenAI example (no API key)")
            print()
        
        # Note: RoBERTa example requires transformers library
        try:
            example_roberta_classifier()
            print()
        except ImportError:
            print("Skipping RoBERTa example (transformers not installed)")
            print()
        
        # Ensemble examples
        if os.getenv("OPENAI_API_KEY") or os.getenv("CLAUDE_API_KEY"):
            example_ensemble_classifier()
            print()
            example_weighted_ensemble()
        else:
            print("Skipping ensemble examples (no API keys)")
    
    except Exception as e:
        print(f"Error running examples: {str(e)}")
        print("Make sure to set your API keys as environment variables:")
        print("  export OPENAI_API_KEY='your-key-here'")
        print("  export CLAUDE_API_KEY='your-key-here'")


if __name__ == "__main__":
    main()



# ./textclassify/examples/multi_label_example.py
"""
Multi-label Text Classification Example

This example demonstrates how to use the textclassify package for multi-label
text classification where each text can have multiple labels.
"""

import os
from textclassify import (
    OpenAIClassifier, GeminiClassifier, RoBERTaClassifier,
    VotingEnsemble, ClassRoutingEnsemble,
    Config, APIKeyManager,
    ClassificationType, ModelType
)
from textclassify.utils import DataLoader, evaluate_predictions, get_data_statistics
from textclassify.core.types import TrainingData, ModelConfig, EnsembleConfig


def create_sample_multilabel_data():
    """Create sample multi-label data for demonstration."""
    texts = [
        "This action movie has amazing special effects and great fight scenes.",
        "A romantic comedy with hilarious moments and heartwarming romance.",
        "Thrilling horror film with jump scares and supernatural elements.",
        "Documentary about science and technology innovations.",
        "Family-friendly animated movie with comedy and adventure.",
        "Dramatic thriller with mystery and suspense elements.",
        "Musical comedy with great songs and funny characters.",
        "Science fiction adventure with action and futuristic themes.",
        "Romantic drama with emotional depth and beautiful cinematography.",
        "Horror comedy that blends scares with humor effectively."
    ]
    
    labels = [
        ["action", "special_effects"],
        ["comedy", "romance"],
        ["horror", "thriller"],
        ["documentary", "science"],
        ["family", "comedy", "adventure"],
        ["drama", "thriller", "mystery"],
        ["comedy", "musical"],
        ["sci_fi", "action", "adventure"],
        ["romance", "drama"],
        ["horror", "comedy"]
    ]
    
    return TrainingData(
        texts=texts,
        labels=labels,
        classification_type=ClassificationType.MULTI_LABEL
    )


def example_multilabel_llm():
    """Example using LLM for multi-label classification."""
    print("=== Multi-label LLM Classification ===")
    
    # Create configuration
    config = ModelConfig(
        model_name="gpt-3.5-turbo",
        model_type=ModelType.LLM,
        api_key=os.getenv("OPENAI_API_KEY"),
        parameters={
            "temperature": 0.1,
            "max_tokens": 100
        }
    )
    
    if not config.api_key:
        print("OpenAI API key not found, skipping example")
        return
    
    # Create classifier
    classifier = OpenAIClassifier(config)
    
    # Create and analyze training data
    training_data = create_sample_multilabel_data()
    stats = get_data_statistics(training_data)
    
    print(f"Dataset statistics:")
    print(f"  Total samples: {stats['total_samples']}")
    print(f"  Number of classes: {stats['num_classes']}")
    print(f"  Average labels per sample: {stats['labels_per_sample']['mean']:.2f}")
    print(f"  Class distribution: {stats['class_distribution']}")
    print()
    
    # Train classifier
    classifier.fit(training_data)
    
    # Test predictions
    test_texts = [
        "An exciting action movie with romance and comedy elements.",
        "Scary horror film with supernatural themes.",
        "Educational documentary about environmental science."
    ]
    
    print("Test predictions:")
    result = classifier.predict(test_texts)
    
    for i, (text, prediction) in enumerate(zip(test_texts, result.predictions)):
        print(f"Text: {text}")
        print(f"Predicted labels: {prediction}")
        print()
    
    # Get predictions with probabilities
    result_proba = classifier.predict_proba(test_texts)
    
    print("Predictions with probabilities:")
    for i, (text, prediction, probabilities) in enumerate(zip(
        test_texts, result_proba.predictions, result_proba.probabilities
    )):
        print(f"Text: {text}")
        print(f"Predicted labels: {prediction}")
        print(f"Probabilities:")
        for label, prob in probabilities.items():
            print(f"  {label}: {prob:.3f}")
        print()


def example_multilabel_roberta():
    """Example using RoBERTa for multi-label classification."""
    print("=== Multi-label RoBERTa Classification ===")
    
    try:
        # Create configuration
        config = ModelConfig(
            model_name="roberta-base",
            model_type=ModelType.TRADITIONAL_ML,
            parameters={
                "max_length": 128,
                "batch_size": 4,
                "num_epochs": 2,
                "learning_rate": 2e-5,
                "threshold": 0.5  # Threshold for multi-label prediction
            }
        )
        
        # Create classifier
        classifier = RoBERTaClassifier(config)
        
        # Create training data
        training_data = create_sample_multilabel_data()
        
        print(f"Training RoBERTa on {len(training_data.texts)} samples...")
        
        # Train the model
        classifier.fit(training_data)
        
        # Test predictions
        test_texts = [
            "Action-packed adventure with comedy elements.",
            "Romantic drama with beautiful cinematography."
        ]
        
        result = classifier.predict(test_texts)
        
        print("RoBERTa predictions:")
        for text, prediction in zip(test_texts, result.predictions):
            print(f"Text: {text}")
            print(f"Predicted labels: {prediction}")
            print()
        
        # Get probabilities
        result_proba = classifier.predict_proba(test_texts)
        
        print("RoBERTa predictions with probabilities:")
        for i, (text, prediction, probabilities) in enumerate(zip(
            test_texts, result_proba.predictions, result_proba.probabilities
        )):
            print(f"Text: {text}")
            print(f"Predicted labels: {prediction}")
            print(f"Top probabilities:")
            # Sort probabilities and show top 5
            sorted_probs = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)
            for label, prob in sorted_probs[:5]:
                print(f"  {label}: {prob:.3f}")
            print()
    
    except ImportError:
        print("RoBERTa example requires transformers library")
        print("Install with: pip install transformers torch")


def example_multilabel_ensemble():
    """Example using ensemble for multi-label classification."""
    print("=== Multi-label Ensemble Classification ===")
    
    # Create model configurations
    openai_config = ModelConfig(
        model_name="gpt-3.5-turbo",
        model_type=ModelType.LLM,
        api_key=os.getenv("OPENAI_API_KEY"),
        parameters={"temperature": 0.1, "max_tokens": 100}
    )
    
    gemini_config = ModelConfig(
        model_name="gemini-1.5-flash",
        model_type=ModelType.LLM,
        api_key=os.getenv("GEMINI_API_KEY"),
        parameters={"temperature": 0.1, "max_tokens": 100}
    )
    
    # Create ensemble configuration
    ensemble_config = EnsembleConfig(
        models=[openai_config, gemini_config],
        ensemble_method="voting"
    )
    
    # Create voting ensemble
    ensemble = VotingEnsemble(ensemble_config)
    
    # Add models if API keys are available
    models_added = 0
    if openai_config.api_key:
        ensemble.add_model(OpenAIClassifier(openai_config), "openai")
        models_added += 1
    if gemini_config.api_key:
        ensemble.add_model(GeminiClassifier(gemini_config), "gemini")
        models_added += 1
    
    if models_added == 0:
        print("No API keys available for ensemble example")
        return
    
    print(f"Created ensemble with {models_added} models")
    
    # Train ensemble
    training_data = create_sample_multilabel_data()
    ensemble.fit(training_data)
    
    # Test predictions
    test_texts = [
        "Funny romantic comedy with great music and songs.",
        "Scary thriller with mystery and supernatural elements."
    ]
    
    result = ensemble.predict(test_texts)
    
    print("Ensemble predictions:")
    for text, prediction in zip(test_texts, result.predictions):
        print(f"Text: {text}")
        print(f"Predicted labels: {prediction}")
        print()
    
    # Show ensemble info
    print(f"Ensemble info:")
    info = ensemble.model_info
    print(f"  Method: {info['ensemble_method']}")
    print(f"  Models: {info['model_names']}")


def example_class_routing_ensemble():
    """Example using class routing ensemble for multi-label classification."""
    print("=== Class Routing Ensemble ===")
    
    # Create model configurations
    model_configs = [
        ModelConfig(
            model_name="gpt-3.5-turbo",
            model_type=ModelType.LLM,
            api_key=os.getenv("OPENAI_API_KEY"),
            parameters={"temperature": 0.1}
        ),
        ModelConfig(
            model_name="gemini-1.5-flash",
            model_type=ModelType.LLM,
            api_key=os.getenv("GEMINI_API_KEY"),
            parameters={"temperature": 0.1}
        )
    ]
    
    # Create routing rules (different models for different classes)
    routing_rules = {
        "action": "openai",
        "comedy": "openai",
        "horror": "gemini",
        "thriller": "gemini",
        "romance": "openai",
        "sci_fi": "gemini"
    }
    
    ensemble_config = EnsembleConfig(
        models=model_configs,
        ensemble_method="routing",
        routing_rules=routing_rules
    )
    
    # Create routing ensemble
    ensemble = ClassRoutingEnsemble(ensemble_config)
    
    # Add models
    models_added = 0
    if model_configs[0].api_key:
        ensemble.add_model(OpenAIClassifier(model_configs[0]), "openai")
        models_added += 1
    if model_configs[1].api_key:
        ensemble.add_model(GeminiClassifier(model_configs[1]), "gemini")
        models_added += 1
    
    if models_added < 2:
        print("Class routing requires multiple API keys")
        return
    
    # Train ensemble
    training_data = create_sample_multilabel_data()
    ensemble.fit(training_data)
    
    # Show routing summary
    print("Routing rules:")
    routing_summary = ensemble.get_routing_summary()
    for class_name, model_name in routing_summary.items():
        print(f"  {class_name} -> {model_name}")
    print()
    
    # Test predictions
    test_texts = [
        "Action-packed sci-fi adventure with thrilling scenes.",
        "Romantic comedy with hilarious moments."
    ]
    
    result = ensemble.predict(test_texts)
    
    print("Class routing predictions:")
    for text, prediction in zip(test_texts, result.predictions):
        print(f"Text: {text}")
        print(f"Predicted labels: {prediction}")
        print()


def example_data_loading():
    """Example of loading multi-label data from files."""
    print("=== Data Loading Example ===")
    
    # Create sample data
    training_data = create_sample_multilabel_data()
    
    # Save to CSV
    csv_path = "sample_multilabel_data.csv"
    DataLoader.save_to_csv(training_data, csv_path)
    print(f"Saved data to {csv_path}")
    
    # Load from CSV
    loaded_data = DataLoader.from_csv(
        csv_path,
        text_column='text',
        label_column='label',
        classification_type=ClassificationType.MULTI_LABEL
    )
    
    print(f"Loaded {len(loaded_data.texts)} samples from CSV")
    
    # Save to JSON
    json_path = "sample_multilabel_data.json"
    DataLoader.save_to_json(training_data, json_path)
    print(f"Saved data to {json_path}")
    
    # Load from JSON
    loaded_json_data = DataLoader.from_json(
        json_path,
        text_field='text',
        label_field='label',
        classification_type=ClassificationType.MULTI_LABEL
    )
    
    print(f"Loaded {len(loaded_json_data.texts)} samples from JSON")
    
    # Show statistics
    stats = get_data_statistics(loaded_data)
    print(f"\nData statistics:")
    print(f"  Classification type: {stats['classification_type']}")
    print(f"  Total samples: {stats['total_samples']}")
    print(f"  Number of classes: {stats['num_classes']}")
    print(f"  Labels per sample: {stats['labels_per_sample']}")


def main():
    """Run all multi-label examples."""
    print("TextClassify Multi-label Classification Examples")
    print("=" * 50)
    
    try:
        # Data loading example
        example_data_loading()
        print()
        
        # LLM examples
        if os.getenv("OPENAI_API_KEY"):
            example_multilabel_llm()
            print()
        else:
            print("Skipping OpenAI example (no API key)")
            print()
        
        # RoBERTa example
        try:
            example_multilabel_roberta()
            print()
        except ImportError:
            print("Skipping RoBERTa example (transformers not installed)")
            print()
        
        # Ensemble examples
        if os.getenv("OPENAI_API_KEY") or os.getenv("GEMINI_API_KEY"):
            example_multilabel_ensemble()
            print()
        
        if os.getenv("OPENAI_API_KEY") and os.getenv("GEMINI_API_KEY"):
            example_class_routing_ensemble()
        else:
            print("Skipping class routing example (requires multiple API keys)")
    
    except Exception as e:
        print(f"Error running examples: {str(e)}")
        print("Make sure to set your API keys as environment variables:")
        print("  export OPENAI_API_KEY='your-key-here'")
        print("  export GEMINI_API_KEY='your-key-here'")


if __name__ == "__main__":
    main()



# ./textclassify/llm/__init__.py
"""LLM-based text classifiers module."""

from .base import BaseLLMClassifier
from .openai_classifier import OpenAIClassifier
from .claude_classifier import ClaudeClassifier
from .gemini_classifier import GeminiClassifier
from .deepseek_classifier import DeepSeekClassifier

__all__ = [
    "BaseLLMClassifier",
    "OpenAIClassifier",
    "ClaudeClassifier", 
    "GeminiClassifier",
    "DeepSeekClassifier",
]



# ./textclassify/llm/base.py
"""Base class for LLM-based text classifiers."""

import asyncio
import json
import re
from typing import Any, Dict, List, Optional, Union

from ..core.base import AsyncBaseClassifier
from ..core.types import ClassificationResult, ClassificationType, ModelType, TrainingData
from ..core.exceptions import PredictionError, ValidationError, APIError
from .prompts import get_prompt_template


class BaseLLMClassifier(AsyncBaseClassifier):
    """Base class for all LLM-based text classifiers."""
    
    def __init__(self, config):
        """Initialize the LLM classifier.
        
        Args:
            config: Model configuration
        """
        super().__init__(config)
        self.config.model_type = ModelType.LLM
        self.examples = []  # Few-shot examples
        
    def fit(self, training_data: TrainingData) -> None:
        """Fit the LLM classifier (stores examples for few-shot learning).
        
        Args:
            training_data: Training data containing texts and labels
        """
        self.classification_type = training_data.classification_type
        
        # Extract unique classes
        if self.classification_type == ClassificationType.MULTI_CLASS:
            self.classes_ = list(set(training_data.labels))
        else:
            # For multi-label, flatten all labels
            all_labels = []
            for label_list in training_data.labels:
                all_labels.extend(label_list)
            self.classes_ = list(set(all_labels))
        
        # Store examples for few-shot learning (limit to avoid token limits)
        max_examples = self.config.parameters.get('max_examples', 5)
        self.examples = []
        
        for i, (text, label) in enumerate(zip(training_data.texts, training_data.labels)):
            if i >= max_examples:
                break
            self.examples.append({'text': text, 'label': label})
        
        self.is_trained = True
    
    def predict(self, texts: List[str]) -> ClassificationResult:
        """Predict labels for texts (synchronous wrapper).
        
        Args:
            texts: List of texts to classify
            
        Returns:
            ClassificationResult with predictions
        """
        return asyncio.run(self.predict_async(texts))
    
    def predict_proba(self, texts: List[str]) -> ClassificationResult:
        """Predict probabilities for texts (synchronous wrapper).
        
        Args:
            texts: List of texts to classify
            
        Returns:
            ClassificationResult with predictions and probabilities
        """
        return asyncio.run(self.predict_proba_async(texts))
    
    async def predict_async(self, texts: List[str]) -> ClassificationResult:
        """Asynchronously predict labels for texts.
        
        Args:
            texts: List of texts to classify
            
        Returns:
            ClassificationResult with predictions
        """
        self.validate_input(texts)
        
        if not self.is_trained:
            raise PredictionError("Model must be trained before prediction", self.config.model_name)
        
        # Get prompt template
        template = get_prompt_template(self.classification_type, with_probabilities=False)
        
        # Process texts in batches
        batch_size = self.config.batch_size
        all_predictions = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            batch_predictions = await self._predict_batch(batch_texts, template)
            all_predictions.extend(batch_predictions)
        
        return self._create_result(predictions=all_predictions)
    
    async def predict_proba_async(self, texts: List[str]) -> ClassificationResult:
        """Asynchronously predict probabilities for texts.
        
        Args:
            texts: List of texts to classify
            
        Returns:
            ClassificationResult with predictions and probabilities
        """
        self.validate_input(texts)
        
        if not self.is_trained:
            raise PredictionError("Model must be trained before prediction", self.config.model_name)
        
        # Get probability prompt template
        template = get_prompt_template(self.classification_type, with_probabilities=True)
        
        # Process texts in batches
        batch_size = self.config.batch_size
        all_predictions = []
        all_probabilities = []
        all_confidence_scores = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            batch_results = await self._predict_proba_batch(batch_texts, template)
            
            for pred, prob, conf in batch_results:
                all_predictions.append(pred)
                all_probabilities.append(prob)
                all_confidence_scores.append(conf)
        
        return self._create_result(
            predictions=all_predictions,
            probabilities=all_probabilities,
            confidence_scores=all_confidence_scores
        )
    
    async def _predict_batch(self, texts: List[str], template) -> List[Union[str, List[str]]]:
        """Predict labels for a batch of texts.
        
        Args:
            texts: Batch of texts to classify
            template: Prompt template to use
            
        Returns:
            List of predictions
        """
        tasks = []
        for text in texts:
            prompt = template.format_prompt(text, self.classes_, self.examples)
            tasks.append(self._call_llm(prompt))
        
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        predictions = []
        for response in responses:
            if isinstance(response, Exception):
                raise PredictionError(f"LLM call failed: {str(response)}", self.config.model_name)
            
            prediction = self._parse_prediction_response(response)
            predictions.append(prediction)
        
        return predictions
    
    async def _predict_proba_batch(self, texts: List[str], template) -> List[tuple]:
        """Predict probabilities for a batch of texts.
        
        Args:
            texts: Batch of texts to classify
            template: Prompt template to use
            
        Returns:
            List of (prediction, probabilities, confidence) tuples
        """
        tasks = []
        for text in texts:
            prompt = template.format_prompt(text, self.classes_, self.examples)
            tasks.append(self._call_llm(prompt))
        
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        results = []
        for response in responses:
            if isinstance(response, Exception):
                raise PredictionError(f"LLM call failed: {str(response)}", self.config.model_name)
            
            prediction, probabilities, confidence = self._parse_probability_response(response)
            results.append((prediction, probabilities, confidence))
        
        return results
    
    def _parse_prediction_response(self, response: str) -> Union[str, List[str]]:
        """Parse the LLM response for predictions.
        
        Args:
            response: Raw LLM response
            
        Returns:
            Parsed prediction(s)
        """
        response = response.strip()
        
        if self.classification_type == ClassificationType.MULTI_CLASS:
            # For multi-class, expect a single class name
            # Clean the response and find the best match
            for class_name in self.classes_:
                if class_name.lower() in response.lower():
                    return class_name
            
            # If no exact match, return the first class as fallback
            return self.classes_[0] if self.classes_ else "unknown"
        
        else:
            # For multi-label, expect comma-separated class names
            if response.upper() == "NONE":
                return []
            
            # Split by comma and clean
            predicted_classes = []
            parts = response.split(',')
            
            for part in parts:
                part = part.strip()
                for class_name in self.classes_:
                    if class_name.lower() in part.lower():
                        predicted_classes.append(class_name)
                        break
            
            return predicted_classes
    
    def _parse_probability_response(self, response: str) -> tuple:
        """Parse the LLM response for probabilities.
        
        Args:
            response: Raw LLM response containing JSON
            
        Returns:
            Tuple of (prediction, probabilities_dict, confidence_score)
        """
        try:
            # Extract JSON from response
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if not json_match:
                raise ValueError("No JSON found in response")
            
            probabilities = json.loads(json_match.group())
            
            # Validate and normalize probabilities
            normalized_probs = {}
            for class_name in self.classes_:
                # Try to find the class in the response (case-insensitive)
                prob = 0.0
                for key, value in probabilities.items():
                    if key.lower() == class_name.lower():
                        prob = float(value)
                        break
                normalized_probs[class_name] = max(0.0, min(1.0, prob))
            
            # Generate prediction based on probabilities
            if self.classification_type == ClassificationType.MULTI_CLASS:
                prediction = max(normalized_probs, key=normalized_probs.get)
                confidence = normalized_probs[prediction]
            else:
                # For multi-label, use threshold (default 0.5)
                threshold = self.config.parameters.get('threshold', 0.5)
                prediction = [cls for cls, prob in normalized_probs.items() if prob >= threshold]
                confidence = max(normalized_probs.values()) if normalized_probs else 0.0
            
            return prediction, normalized_probs, confidence
            
        except (json.JSONDecodeError, ValueError, KeyError) as e:
            # Fallback to simple prediction parsing
            prediction = self._parse_prediction_response(response)
            
            # Create uniform probabilities
            if self.classification_type == ClassificationType.MULTI_CLASS:
                prob_value = 1.0 / len(self.classes_) if self.classes_ else 0.0
                probabilities = {cls: prob_value for cls in self.classes_}
                confidence = prob_value
            else:
                probabilities = {cls: 0.5 for cls in self.classes_}
                confidence = 0.5
            
            return prediction, probabilities, confidence
    
    async def _call_llm(self, prompt: str) -> str:
        """Call the LLM API with the given prompt.
        
        This method should be implemented by each specific LLM provider.
        
        Args:
            prompt: The prompt to send to the LLM
            
        Returns:
            The LLM's response
            
        Raises:
            APIError: If the API call fails
        """
        raise NotImplementedError("Subclasses must implement _call_llm method")



# ./textclassify/llm/claude_classifier.py
"""Claude (Anthropic) based text classifier."""

import asyncio
import aiohttp
import json
from typing import Dict, Any

from ..core.exceptions import APIError, ConfigurationError
from .base import BaseLLMClassifier


class ClaudeClassifier(BaseLLMClassifier):
    """Text classifier using Anthropic's Claude models."""
    
    def __init__(self, config):
        """Initialize Claude classifier.
        
        Args:
            config: Model configuration with Claude-specific parameters
        """
        super().__init__(config)
        
        # Validate required configuration
        if not self.config.api_key:
            raise ConfigurationError("Claude API key is required")
        
        # Set default parameters
        self.model = self.config.parameters.get('model', 'claude-3-haiku-20240307')
        self.max_tokens = self.config.parameters.get('max_tokens', 150)
        self.temperature = self.config.parameters.get('temperature', 0.1)
        self.api_base = self.config.api_base or "https://api.anthropic.com"
        
        # Headers for API requests
        self.headers = {
            "x-api-key": self.config.api_key,
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01"
        }
    
    async def _call_llm(self, prompt: str) -> str:
        """Call Claude API with the given prompt.
        
        Args:
            prompt: The prompt to send to Claude
            
        Returns:
            The model's response
            
        Raises:
            APIError: If the API call fails
        """
        url = f"{self.api_base}/v1/messages"
        
        payload = {
            "model": self.model,
            "max_tokens": self.max_tokens,
            "temperature": self.temperature,
            "messages": [
                {"role": "user", "content": prompt}
            ]
        }
        
        # Add additional parameters if specified
        if 'top_p' in self.config.parameters:
            payload['top_p'] = self.config.parameters['top_p']
        if 'top_k' in self.config.parameters:
            payload['top_k'] = self.config.parameters['top_k']
        if 'system' in self.config.parameters:
            payload['system'] = self.config.parameters['system']
        
        for attempt in range(self.config.max_retries):
            try:
                async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=self.config.timeout)) as session:
                    async with session.post(url, headers=self.headers, json=payload) as response:
                        response_data = await response.json()
                        
                        if response.status == 200:
                            # Claude returns content in a different format
                            content = response_data.get('content', [])
                            if content and len(content) > 0:
                                return content[0].get('text', '').strip()
                            else:
                                raise APIError(
                                    "Empty response from Claude",
                                    provider="claude",
                                    status_code=response.status
                                )
                        
                        elif response.status == 429:  # Rate limit
                            if attempt < self.config.max_retries - 1:
                                wait_time = 2 ** attempt  # Exponential backoff
                                await asyncio.sleep(wait_time)
                                continue
                            else:
                                raise APIError(
                                    f"Rate limit exceeded after {self.config.max_retries} attempts",
                                    provider="claude",
                                    status_code=response.status
                                )
                        
                        elif response.status == 401:
                            raise APIError(
                                "Invalid API key",
                                provider="claude",
                                status_code=response.status
                            )
                        
                        elif response.status == 400:
                            error_msg = response_data.get('error', {}).get('message', 'Bad request')
                            raise APIError(
                                f"Bad request: {error_msg}",
                                provider="claude",
                                status_code=response.status
                            )
                        
                        else:
                            error_msg = response_data.get('error', {}).get('message', 'Unknown error')
                            raise APIError(
                                f"API error: {error_msg}",
                                provider="claude",
                                status_code=response.status
                            )
            
            except aiohttp.ClientError as e:
                if attempt < self.config.max_retries - 1:
                    wait_time = 2 ** attempt
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    raise APIError(
                        f"Network error after {self.config.max_retries} attempts: {str(e)}",
                        provider="claude"
                    )
            
            except asyncio.TimeoutError:
                if attempt < self.config.max_retries - 1:
                    wait_time = 2 ** attempt
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    raise APIError(
                        f"Request timeout after {self.config.max_retries} attempts",
                        provider="claude"
                    )
        
        raise APIError(
            f"Failed to get response after {self.config.max_retries} attempts",
            provider="claude"
        )
    
    @property
    def model_info(self) -> Dict[str, Any]:
        """Get Claude model information."""
        info = super().model_info
        info.update({
            "provider": "claude",
            "model": self.model,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "api_base": self.api_base
        })
        return info



# ./textclassify/llm/deepseek_classifier.py
"""DeepSeek based text classifier."""

import asyncio
import aiohttp
import json
from typing import Dict, Any

from ..core.exceptions import APIError, ConfigurationError
from .base import BaseLLMClassifier


class DeepSeekClassifier(BaseLLMClassifier):
    """Text classifier using DeepSeek models."""
    
    def __init__(self, config):
        """Initialize DeepSeek classifier.
        
        Args:
            config: Model configuration with DeepSeek-specific parameters
        """
        super().__init__(config)
        
        # Validate required configuration
        if not self.config.api_key:
            raise ConfigurationError("DeepSeek API key is required")
        
        # Set default parameters
        self.model = self.config.parameters.get('model', 'deepseek-chat')
        self.temperature = self.config.parameters.get('temperature', 0.1)
        self.max_tokens = self.config.parameters.get('max_tokens', 150)
        self.api_base = self.config.api_base or "https://api.deepseek.com"
        
        # Headers for API requests (DeepSeek uses OpenAI-compatible API)
        self.headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json"
        }
    
    async def _call_llm(self, prompt: str) -> str:
        """Call DeepSeek API with the given prompt.
        
        Args:
            prompt: The prompt to send to DeepSeek
            
        Returns:
            The model's response
            
        Raises:
            APIError: If the API call fails
        """
        url = f"{self.api_base}/v1/chat/completions"
        
        payload = {
            "model": self.model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "stream": False
        }
        
        # Add additional parameters if specified
        if 'top_p' in self.config.parameters:
            payload['top_p'] = self.config.parameters['top_p']
        if 'frequency_penalty' in self.config.parameters:
            payload['frequency_penalty'] = self.config.parameters['frequency_penalty']
        if 'presence_penalty' in self.config.parameters:
            payload['presence_penalty'] = self.config.parameters['presence_penalty']
        if 'stop' in self.config.parameters:
            payload['stop'] = self.config.parameters['stop']
        
        for attempt in range(self.config.max_retries):
            try:
                async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=self.config.timeout)) as session:
                    async with session.post(url, headers=self.headers, json=payload) as response:
                        response_data = await response.json()
                        
                        if response.status == 200:
                            choices = response_data.get('choices', [])
                            if choices and len(choices) > 0:
                                message = choices[0].get('message', {})
                                content = message.get('content', '').strip()
                                if content:
                                    return content
                                else:
                                    raise APIError(
                                        "Empty content in DeepSeek response",
                                        provider="deepseek",
                                        status_code=response.status
                                    )
                            else:
                                raise APIError(
                                    "No choices in DeepSeek response",
                                    provider="deepseek",
                                    status_code=response.status
                                )
                        
                        elif response.status == 429:  # Rate limit
                            if attempt < self.config.max_retries - 1:
                                wait_time = 2 ** attempt  # Exponential backoff
                                await asyncio.sleep(wait_time)
                                continue
                            else:
                                raise APIError(
                                    f"Rate limit exceeded after {self.config.max_retries} attempts",
                                    provider="deepseek",
                                    status_code=response.status
                                )
                        
                        elif response.status == 401:
                            raise APIError(
                                "Invalid API key",
                                provider="deepseek",
                                status_code=response.status
                            )
                        
                        elif response.status == 400:
                            error_msg = response_data.get('error', {}).get('message', 'Bad request')
                            raise APIError(
                                f"Bad request: {error_msg}",
                                provider="deepseek",
                                status_code=response.status
                            )
                        
                        else:
                            error_msg = response_data.get('error', {}).get('message', 'Unknown error')
                            raise APIError(
                                f"API error: {error_msg}",
                                provider="deepseek",
                                status_code=response.status
                            )
            
            except aiohttp.ClientError as e:
                if attempt < self.config.max_retries - 1:
                    wait_time = 2 ** attempt
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    raise APIError(
                        f"Network error after {self.config.max_retries} attempts: {str(e)}",
                        provider="deepseek"
                    )
            
            except asyncio.TimeoutError:
                if attempt < self.config.max_retries - 1:
                    wait_time = 2 ** attempt
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    raise APIError(
                        f"Request timeout after {self.config.max_retries} attempts",
                        provider="deepseek"
                    )
        
        raise APIError(
            f"Failed to get response after {self.config.max_retries} attempts",
            provider="deepseek"
        )
    
    @property
    def model_info(self) -> Dict[str, Any]:
        """Get DeepSeek model information."""
        info = super().model_info
        info.update({
            "provider": "deepseek",
            "model": self.model,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "api_base": self.api_base
        })
        return info



# ./textclassify/llm/gemini_classifier.py
"""Google Gemini based text classifier."""

import asyncio
import aiohttp
import json
from typing import Dict, Any

from ..core.exceptions import APIError, ConfigurationError
from .base import BaseLLMClassifier


class GeminiClassifier(BaseLLMClassifier):
    """Text classifier using Google's Gemini models."""
    
    def __init__(self, config):
        """Initialize Gemini classifier.
        
        Args:
            config: Model configuration with Gemini-specific parameters
        """
        super().__init__(config)
        
        # Validate required configuration
        if not self.config.api_key:
            raise ConfigurationError("Gemini API key is required")
        
        # Set default parameters
        self.model = self.config.parameters.get('model', 'gemini-1.5-flash')
        self.temperature = self.config.parameters.get('temperature', 0.1)
        self.max_tokens = self.config.parameters.get('max_tokens', 150)
        self.api_base = self.config.api_base or "https://generativelanguage.googleapis.com"
        
        # Gemini uses different parameter names
        self.top_p = self.config.parameters.get('top_p', 0.95)
        self.top_k = self.config.parameters.get('top_k', 40)
    
    async def _call_llm(self, prompt: str) -> str:
        """Call Gemini API with the given prompt.
        
        Args:
            prompt: The prompt to send to Gemini
            
        Returns:
            The model's response
            
        Raises:
            APIError: If the API call fails
        """
        url = f"{self.api_base}/v1beta/models/{self.model}:generateContent"
        
        payload = {
            "contents": [
                {
                    "parts": [
                        {"text": prompt}
                    ]
                }
            ],
            "generationConfig": {
                "temperature": self.temperature,
                "topP": self.top_p,
                "topK": self.top_k,
                "maxOutputTokens": self.max_tokens,
                "candidateCount": 1
            }
        }
        
        # Add safety settings if specified
        if 'safety_settings' in self.config.parameters:
            payload['safetySettings'] = self.config.parameters['safety_settings']
        
        params = {"key": self.config.api_key}
        
        for attempt in range(self.config.max_retries):
            try:
                async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=self.config.timeout)) as session:
                    async with session.post(url, params=params, json=payload) as response:
                        response_data = await response.json()
                        
                        if response.status == 200:
                            # Extract text from Gemini response format
                            candidates = response_data.get('candidates', [])
                            if candidates and len(candidates) > 0:
                                content = candidates[0].get('content', {})
                                parts = content.get('parts', [])
                                if parts and len(parts) > 0:
                                    return parts[0].get('text', '').strip()
                                else:
                                    raise APIError(
                                        "No text content in Gemini response",
                                        provider="gemini",
                                        status_code=response.status
                                    )
                            else:
                                # Check for safety filter or other issues
                                if 'promptFeedback' in response_data:
                                    feedback = response_data['promptFeedback']
                                    if feedback.get('blockReason'):
                                        raise APIError(
                                            f"Content blocked by safety filter: {feedback.get('blockReason')}",
                                            provider="gemini",
                                            status_code=response.status
                                        )
                                
                                raise APIError(
                                    "Empty response from Gemini",
                                    provider="gemini",
                                    status_code=response.status
                                )
                        
                        elif response.status == 429:  # Rate limit
                            if attempt < self.config.max_retries - 1:
                                wait_time = 2 ** attempt  # Exponential backoff
                                await asyncio.sleep(wait_time)
                                continue
                            else:
                                raise APIError(
                                    f"Rate limit exceeded after {self.config.max_retries} attempts",
                                    provider="gemini",
                                    status_code=response.status
                                )
                        
                        elif response.status == 400:
                            error_msg = response_data.get('error', {}).get('message', 'Bad request')
                            raise APIError(
                                f"Bad request: {error_msg}",
                                provider="gemini",
                                status_code=response.status
                            )
                        
                        elif response.status == 403:
                            raise APIError(
                                "Invalid API key or insufficient permissions",
                                provider="gemini",
                                status_code=response.status
                            )
                        
                        else:
                            error_msg = response_data.get('error', {}).get('message', 'Unknown error')
                            raise APIError(
                                f"API error: {error_msg}",
                                provider="gemini",
                                status_code=response.status
                            )
            
            except aiohttp.ClientError as e:
                if attempt < self.config.max_retries - 1:
                    wait_time = 2 ** attempt
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    raise APIError(
                        f"Network error after {self.config.max_retries} attempts: {str(e)}",
                        provider="gemini"
                    )
            
            except asyncio.TimeoutError:
                if attempt < self.config.max_retries - 1:
                    wait_time = 2 ** attempt
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    raise APIError(
                        f"Request timeout after {self.config.max_retries} attempts",
                        provider="gemini"
                    )
        
        raise APIError(
            f"Failed to get response after {self.config.max_retries} attempts",
            provider="gemini"
        )
    
    @property
    def model_info(self) -> Dict[str, Any]:
        """Get Gemini model information."""
        info = super().model_info
        info.update({
            "provider": "gemini",
            "model": self.model,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "top_p": self.top_p,
            "top_k": self.top_k,
            "api_base": self.api_base
        })
        return info



# ./textclassify/llm/openai_classifier.py
"""OpenAI-based text classifier."""

import asyncio
import aiohttp
import json
from typing import Dict, Any

from ..core.exceptions import APIError, ConfigurationError
from .base import BaseLLMClassifier


class OpenAIClassifier(BaseLLMClassifier):
    """Text classifier using OpenAI's GPT models."""
    
    def __init__(self, config):
        """Initialize OpenAI classifier.
        
        Args:
            config: Model configuration with OpenAI-specific parameters
        """
        super().__init__(config)
        
        # Validate required configuration
        if not self.config.api_key:
            raise ConfigurationError("OpenAI API key is required")
        
        # Set default parameters
        self.model = self.config.parameters.get('model', 'gpt-3.5-turbo')
        self.temperature = self.config.parameters.get('temperature', 0.1)
        self.max_tokens = self.config.parameters.get('max_tokens', 150)
        self.api_base = self.config.api_base or "https://api.openai.com/v1"
        
        # Headers for API requests
        self.headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json"
        }
    
    async def _call_llm(self, prompt: str) -> str:
        """Call OpenAI API with the given prompt.
        
        Args:
            prompt: The prompt to send to OpenAI
            
        Returns:
            The model's response
            
        Raises:
            APIError: If the API call fails
        """
        url = f"{self.api_base}/chat/completions"
        
        payload = {
            "model": self.model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "temperature": self.temperature,
            "max_tokens": self.max_tokens
        }
        
        # Add additional parameters if specified
        if 'top_p' in self.config.parameters:
            payload['top_p'] = self.config.parameters['top_p']
        if 'frequency_penalty' in self.config.parameters:
            payload['frequency_penalty'] = self.config.parameters['frequency_penalty']
        if 'presence_penalty' in self.config.parameters:
            payload['presence_penalty'] = self.config.parameters['presence_penalty']
        
        for attempt in range(self.config.max_retries):
            try:
                async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=self.config.timeout)) as session:
                    async with session.post(url, headers=self.headers, json=payload) as response:
                        response_data = await response.json()
                        
                        if response.status == 200:
                            return response_data['choices'][0]['message']['content'].strip()
                        
                        elif response.status == 429:  # Rate limit
                            if attempt < self.config.max_retries - 1:
                                wait_time = 2 ** attempt  # Exponential backoff
                                await asyncio.sleep(wait_time)
                                continue
                            else:
                                raise APIError(
                                    f"Rate limit exceeded after {self.config.max_retries} attempts",
                                    provider="openai",
                                    status_code=response.status
                                )
                        
                        elif response.status == 401:
                            raise APIError(
                                "Invalid API key",
                                provider="openai",
                                status_code=response.status
                            )
                        
                        elif response.status == 400:
                            error_msg = response_data.get('error', {}).get('message', 'Bad request')
                            raise APIError(
                                f"Bad request: {error_msg}",
                                provider="openai",
                                status_code=response.status
                            )
                        
                        else:
                            error_msg = response_data.get('error', {}).get('message', 'Unknown error')
                            raise APIError(
                                f"API error: {error_msg}",
                                provider="openai",
                                status_code=response.status
                            )
            
            except aiohttp.ClientError as e:
                if attempt < self.config.max_retries - 1:
                    wait_time = 2 ** attempt
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    raise APIError(
                        f"Network error after {self.config.max_retries} attempts: {str(e)}",
                        provider="openai"
                    )
            
            except asyncio.TimeoutError:
                if attempt < self.config.max_retries - 1:
                    wait_time = 2 ** attempt
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    raise APIError(
                        f"Request timeout after {self.config.max_retries} attempts",
                        provider="openai"
                    )
        
        raise APIError(
            f"Failed to get response after {self.config.max_retries} attempts",
            provider="openai"
        )
    
    @property
    def model_info(self) -> Dict[str, Any]:
        """Get OpenAI model information."""
        info = super().model_info
        info.update({
            "provider": "openai",
            "model": self.model,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "api_base": self.api_base
        })
        return info



# ./textclassify/llm/prompts.py
"""Prompt templates for LLM-based text classification."""

from typing import List, Dict, Any
from ..core.types import ClassificationType


class PromptTemplate:
    """Base class for prompt templates."""
    
    def __init__(self, classification_type: ClassificationType):
        self.classification_type = classification_type
    
    def format_prompt(self, text: str, classes: List[str], examples: List[Dict[str, Any]] = None) -> str:
        """Format the prompt for classification.
        
        Args:
            text: Text to classify
            classes: List of possible classes
            examples: Optional few-shot examples
            
        Returns:
            Formatted prompt string
        """
        raise NotImplementedError


class MultiClassPromptTemplate(PromptTemplate):
    """Prompt template for multi-class classification."""
    
    def __init__(self):
        super().__init__(ClassificationType.MULTI_CLASS)
    
    def format_prompt(self, text: str, classes: List[str], examples: List[Dict[str, Any]] = None) -> str:
        """Format prompt for multi-class classification."""
        
        # Base instruction
        prompt = f"""You are a text classification expert. Your task is to classify the given text into exactly ONE of the following categories:

Categories:
{self._format_classes(classes)}

Instructions:
1. Read the text carefully
2. Choose the SINGLE most appropriate category
3. Respond with ONLY the category name, nothing else
4. If uncertain, choose the closest match

"""
        
        # Add examples if provided
        if examples:
            prompt += "Examples:\n"
            for example in examples:
                prompt += f"Text: {example['text']}\nCategory: {example['label']}\n\n"
        
        # Add the text to classify
        prompt += f"Text to classify: {text}\nCategory:"
        
        return prompt
    
    def _format_classes(self, classes: List[str]) -> str:
        """Format the list of classes."""
        return "\n".join(f"- {cls}" for cls in classes)


class MultiLabelPromptTemplate(PromptTemplate):
    """Prompt template for multi-label classification."""
    
    def __init__(self):
        super().__init__(ClassificationType.MULTI_LABEL)
    
    def format_prompt(self, text: str, classes: List[str], examples: List[Dict[str, Any]] = None) -> str:
        """Format prompt for multi-label classification."""
        
        # Base instruction
        prompt = f"""You are a text classification expert. Your task is to classify the given text into one or more of the following categories:

Categories:
{self._format_classes(classes)}

Instructions:
1. Read the text carefully
2. Select ALL categories that apply to the text
3. You can select multiple categories if they all apply
4. If no categories apply, respond with "NONE"
5. Respond with category names separated by commas, nothing else

"""
        
        # Add examples if provided
        if examples:
            prompt += "Examples:\n"
            for example in examples:
                labels = ", ".join(example['label']) if isinstance(example['label'], list) else example['label']
                prompt += f"Text: {example['text']}\nCategories: {labels}\n\n"
        
        # Add the text to classify
        prompt += f"Text to classify: {text}\nCategories:"
        
        return prompt
    
    def _format_classes(self, classes: List[str]) -> str:
        """Format the list of classes."""
        return "\n".join(f"- {cls}" for cls in classes)


class ProbabilityPromptTemplate(PromptTemplate):
    """Prompt template for getting classification probabilities."""
    
    def __init__(self, classification_type: ClassificationType):
        super().__init__(classification_type)
    
    def format_prompt(self, text: str, classes: List[str], examples: List[Dict[str, Any]] = None) -> str:
        """Format prompt for probability-based classification."""
        
        if self.classification_type == ClassificationType.MULTI_CLASS:
            return self._format_multiclass_probability_prompt(text, classes, examples)
        else:
            return self._format_multilabel_probability_prompt(text, classes, examples)
    
    def _format_multiclass_probability_prompt(self, text: str, classes: List[str], examples: List[Dict[str, Any]] = None) -> str:
        """Format prompt for multi-class probability classification."""
        
        prompt = f"""You are a text classification expert. Your task is to classify the given text and provide confidence scores for each category.

Categories:
{self._format_classes(classes)}

Instructions:
1. Read the text carefully
2. Assign a confidence score (0.0 to 1.0) for each category
3. Scores should sum to 1.0
4. Respond in JSON format: {{"category1": score1, "category2": score2, ...}}
5. Use exactly the category names provided

"""
        
        # Add examples if provided
        if examples:
            prompt += "Examples:\n"
            for example in examples:
                prompt += f"Text: {example['text']}\nScores: {example.get('scores', 'N/A')}\n\n"
        
        # Add the text to classify
        prompt += f"Text to classify: {text}\nScores:"
        
        return prompt
    
    def _format_multilabel_probability_prompt(self, text: str, classes: List[str], examples: List[Dict[str, Any]] = None) -> str:
        """Format prompt for multi-label probability classification."""
        
        prompt = f"""You are a text classification expert. Your task is to classify the given text and provide confidence scores for each category.

Categories:
{self._format_classes(classes)}

Instructions:
1. Read the text carefully
2. Assign a confidence score (0.0 to 1.0) for each category independently
3. Each score represents how likely the category applies to the text
4. Scores do NOT need to sum to 1.0 (this is multi-label classification)
5. Respond in JSON format: {{"category1": score1, "category2": score2, ...}}
6. Use exactly the category names provided

"""
        
        # Add examples if provided
        if examples:
            prompt += "Examples:\n"
            for example in examples:
                prompt += f"Text: {example['text']}\nScores: {example.get('scores', 'N/A')}\n\n"
        
        # Add the text to classify
        prompt += f"Text to classify: {text}\nScores:"
        
        return prompt
    
    def _format_classes(self, classes: List[str]) -> str:
        """Format the list of classes."""
        return "\n".join(f"- {cls}" for cls in classes)


def get_prompt_template(classification_type: ClassificationType, with_probabilities: bool = False) -> PromptTemplate:
    """Get the appropriate prompt template.
    
    Args:
        classification_type: Type of classification (multi-class or multi-label)
        with_probabilities: Whether to request probability scores
        
    Returns:
        Appropriate prompt template instance
    """
    if with_probabilities:
        return ProbabilityPromptTemplate(classification_type)
    elif classification_type == ClassificationType.MULTI_CLASS:
        return MultiClassPromptTemplate()
    else:
        return MultiLabelPromptTemplate()



# ./textclassify/ml/__init__.py
"""Traditional machine learning classifiers module."""

from .base import BaseMLClassifier
from .roberta_classifier import RoBERTaClassifier
from .preprocessing import TextPreprocessor

__all__ = [
    "BaseMLClassifier",
    "RoBERTaClassifier",
    "TextPreprocessor",
]



# ./textclassify/ml/base.py
"""Base class for traditional machine learning classifiers."""

import pickle
import os
from typing import Any, Dict, List, Optional, Union

from ..core.base import BaseClassifier
from ..core.types import ClassificationResult, ClassificationType, ModelType, TrainingData
from ..core.exceptions import ModelTrainingError, PredictionError, ValidationError


class BaseMLClassifier(BaseClassifier):
    """Base class for traditional machine learning text classifiers."""
    
    def __init__(self, config):
        """Initialize the ML classifier.
        
        Args:
            config: Model configuration
        """
        super().__init__(config)
        self.config.model_type = ModelType.TRADITIONAL_ML
        self.model = None
        self.tokenizer = None
        self.label_encoder = None
        self.model_path = None
        
    def save_model(self, path: str) -> None:
        """Save the trained model to disk.
        
        Args:
            path: Path to save the model
            
        Raises:
            ModelTrainingError: If model is not trained or save fails
        """
        if not self.is_trained:
            raise ModelTrainingError("Model must be trained before saving", self.config.model_name)
        
        try:
            model_data = {
                'model': self.model,
                'tokenizer': self.tokenizer,
                'label_encoder': self.label_encoder,
                'classes_': self.classes_,
                'classification_type': self.classification_type,
                'config': self.config,
                'is_trained': self.is_trained
            }
            
            os.makedirs(os.path.dirname(path), exist_ok=True)
            with open(path, 'wb') as f:
                pickle.dump(model_data, f)
            
            self.model_path = path
            
        except Exception as e:
            raise ModelTrainingError(f"Failed to save model: {str(e)}", self.config.model_name)
    
    def load_model(self, path: str) -> None:
        """Load a trained model from disk.
        
        Args:
            path: Path to the saved model
            
        Raises:
            ModelTrainingError: If model loading fails
        """
        try:
            with open(path, 'rb') as f:
                model_data = pickle.load(f)
            
            self.model = model_data['model']
            self.tokenizer = model_data['tokenizer']
            self.label_encoder = model_data['label_encoder']
            self.classes_ = model_data['classes_']
            self.classification_type = model_data['classification_type']
            self.is_trained = model_data['is_trained']
            self.model_path = path
            
            # Update config if needed
            if 'config' in model_data:
                saved_config = model_data['config']
                # Merge saved config with current config, prioritizing current config
                for key, value in saved_config.parameters.items():
                    if key not in self.config.parameters:
                        self.config.parameters[key] = value
            
        except Exception as e:
            raise ModelTrainingError(f"Failed to load model: {str(e)}", self.config.model_name)
    
    def _prepare_labels(self, labels: Union[List[str], List[List[str]]], classification_type: ClassificationType):
        """Prepare labels for training.
        
        Args:
            labels: Raw labels
            classification_type: Type of classification
            
        Returns:
            Processed labels suitable for training
        """
        if classification_type == ClassificationType.MULTI_CLASS:
            # For multi-class, labels are already in the right format
            return labels
        else:
            # For multi-label, we need to handle the format differently
            # This will be implemented by specific classifiers
            return labels
    
    def _validate_training_data(self, training_data: TrainingData) -> None:
        """Validate training data.
        
        Args:
            training_data: Training data to validate
            
        Raises:
            ValidationError: If validation fails
        """
        if not training_data.texts:
            raise ValidationError("Training texts cannot be empty")
        
        if not training_data.labels:
            raise ValidationError("Training labels cannot be empty")
        
        if len(training_data.texts) != len(training_data.labels):
            raise ValidationError("Number of texts and labels must match")
        
        # Check for empty texts
        if any(not text.strip() for text in training_data.texts):
            raise ValidationError("Training texts cannot be empty or whitespace only")
        
        # Validate label format based on classification type
        if training_data.classification_type == ClassificationType.MULTI_CLASS:
            if not all(isinstance(label, str) for label in training_data.labels):
                raise ValidationError("Multi-class labels must be strings")
        else:
            if not all(isinstance(label, list) for label in training_data.labels):
                raise ValidationError("Multi-label labels must be lists of strings")
            
            # Check for empty label lists
            if any(not label for label in training_data.labels):
                raise ValidationError("Multi-label training requires at least one label per text")
    
    @property
    def model_info(self) -> Dict[str, Any]:
        """Get ML model information."""
        info = super().model_info
        info.update({
            "model_path": self.model_path,
            "has_tokenizer": self.tokenizer is not None,
            "has_label_encoder": self.label_encoder is not None
        })
        return info



# ./textclassify/ml/preprocessing.py
"""Text preprocessing utilities for ML classifiers."""

import re
import string
from typing import List, Optional, Dict, Any


class TextPreprocessor:
    """Text preprocessing utilities for machine learning models."""
    
    def __init__(self, 
                 lowercase: bool = True,
                 remove_punctuation: bool = False,
                 remove_numbers: bool = False,
                 remove_extra_whitespace: bool = True,
                 min_length: int = 1,
                 max_length: Optional[int] = None):
        """Initialize text preprocessor.
        
        Args:
            lowercase: Whether to convert text to lowercase
            remove_punctuation: Whether to remove punctuation
            remove_numbers: Whether to remove numbers
            remove_extra_whitespace: Whether to remove extra whitespace
            min_length: Minimum text length (in characters)
            max_length: Maximum text length (in characters)
        """
        self.lowercase = lowercase
        self.remove_punctuation = remove_punctuation
        self.remove_numbers = remove_numbers
        self.remove_extra_whitespace = remove_extra_whitespace
        self.min_length = min_length
        self.max_length = max_length
        
        # Compile regex patterns for efficiency
        self.number_pattern = re.compile(r'\d+')
        self.whitespace_pattern = re.compile(r'\s+')
        self.punctuation_pattern = re.compile(f'[{re.escape(string.punctuation)}]')
    
    def preprocess_text(self, text: str) -> str:
        """Preprocess a single text.
        
        Args:
            text: Input text to preprocess
            
        Returns:
            Preprocessed text
        """
        if not isinstance(text, str):
            text = str(text)
        
        # Convert to lowercase
        if self.lowercase:
            text = text.lower()
        
        # Remove numbers
        if self.remove_numbers:
            text = self.number_pattern.sub(' ', text)
        
        # Remove punctuation
        if self.remove_punctuation:
            text = self.punctuation_pattern.sub(' ', text)
        
        # Remove extra whitespace
        if self.remove_extra_whitespace:
            text = self.whitespace_pattern.sub(' ', text)
        
        # Strip leading/trailing whitespace
        text = text.strip()
        
        # Apply length constraints
        if len(text) < self.min_length:
            return ""  # Return empty string for texts that are too short
        
        if self.max_length and len(text) > self.max_length:
            text = text[:self.max_length]
        
        return text
    
    def preprocess_texts(self, texts: List[str]) -> List[str]:
        """Preprocess a list of texts.
        
        Args:
            texts: List of input texts
            
        Returns:
            List of preprocessed texts
        """
        return [self.preprocess_text(text) for text in texts]
    
    def get_config(self) -> Dict[str, Any]:
        """Get preprocessor configuration.
        
        Returns:
            Dictionary containing preprocessor settings
        """
        return {
            'lowercase': self.lowercase,
            'remove_punctuation': self.remove_punctuation,
            'remove_numbers': self.remove_numbers,
            'remove_extra_whitespace': self.remove_extra_whitespace,
            'min_length': self.min_length,
            'max_length': self.max_length
        }
    
    @classmethod
    def from_config(cls, config: Dict[str, Any]) -> 'TextPreprocessor':
        """Create preprocessor from configuration.
        
        Args:
            config: Configuration dictionary
            
        Returns:
            TextPreprocessor instance
        """
        return cls(**config)


def clean_text(text: str, 
               remove_urls: bool = True,
               remove_emails: bool = True,
               remove_mentions: bool = True,
               remove_hashtags: bool = False) -> str:
    """Clean text by removing specific patterns.
    
    Args:
        text: Input text
        remove_urls: Whether to remove URLs
        remove_emails: Whether to remove email addresses
        remove_mentions: Whether to remove @mentions
        remove_hashtags: Whether to remove #hashtags
        
    Returns:
        Cleaned text
    """
    if not isinstance(text, str):
        text = str(text)
    
    # Remove URLs
    if remove_urls:
        url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
        text = url_pattern.sub(' ', text)
        
        # Also remove www.domain.com patterns
        www_pattern = re.compile(r'www\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
        text = www_pattern.sub(' ', text)
    
    # Remove email addresses
    if remove_emails:
        email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
        text = email_pattern.sub(' ', text)
    
    # Remove @mentions
    if remove_mentions:
        mention_pattern = re.compile(r'@\w+')
        text = mention_pattern.sub(' ', text)
    
    # Remove #hashtags
    if remove_hashtags:
        hashtag_pattern = re.compile(r'#\w+')
        text = hashtag_pattern.sub(' ', text)
    
    # Clean up extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text


def normalize_text(text: str) -> str:
    """Normalize text for better processing.
    
    Args:
        text: Input text
        
    Returns:
        Normalized text
    """
    if not isinstance(text, str):
        text = str(text)
    
    # Replace common contractions
    contractions = {
        "won't": "will not",
        "can't": "cannot",
        "n't": " not",
        "'re": " are",
        "'ve": " have",
        "'ll": " will",
        "'d": " would",
        "'m": " am"
    }
    
    for contraction, expansion in contractions.items():
        text = text.replace(contraction, expansion)
    
    # Normalize quotes
    text = text.replace('"', '"').replace('"', '"')
    text = text.replace(''', "'").replace(''', "'")
    
    # Normalize dashes
    text = text.replace('—', '-').replace('–', '-')
    
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text



# ./textclassify/ml/roberta_classifier.py
"""RoBERTa-based text classifier using transformers."""

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from typing import Any, Dict, List, Optional, Union
import warnings

from ..core.types import ClassificationResult, ClassificationType, TrainingData
from ..core.exceptions import ModelTrainingError, PredictionError, ValidationError
from .base import BaseMLClassifier
from .preprocessing import TextPreprocessor, clean_text, normalize_text

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore", category=UserWarning)

try:
    from transformers import (
        RobertaTokenizer, 
        RobertaForSequenceClassification,
        RobertaModel,
        AdamW,
        get_linear_schedule_with_warmup
    )
    from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder
    from sklearn.metrics import accuracy_score, f1_score
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False


class TextDataset(Dataset):
    """Dataset class for text classification."""
    
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(self.labels[idx], dtype=torch.long)
        }


class RoBERTaClassifier(BaseMLClassifier):
    """RoBERTa-based text classifier."""
    
    def __init__(self, config):
        """Initialize RoBERTa classifier.
        
        Args:
            config: Model configuration
        """
        if not TRANSFORMERS_AVAILABLE:
            raise ImportError(
                "transformers and scikit-learn are required for RoBERTa classifier. "
                "Install with: pip install transformers torch scikit-learn"
            )
        
        super().__init__(config)
        
        # Model parameters
        self.model_name = self.config.parameters.get('model_name', 'roberta-base')
        self.max_length = self.config.parameters.get('max_length', 512)
        self.batch_size = self.config.parameters.get('batch_size', 16)
        self.learning_rate = self.config.parameters.get('learning_rate', 2e-5)
        self.num_epochs = self.config.parameters.get('num_epochs', 3)
        self.warmup_steps = self.config.parameters.get('warmup_steps', 0)
        self.weight_decay = self.config.parameters.get('weight_decay', 0.01)
        
        # Preprocessing
        preprocessing_config = self.config.parameters.get('preprocessing', {})
        self.preprocessor = TextPreprocessor(**preprocessing_config)
        
        # Device
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Initialize components
        self.tokenizer = None
        self.model = None
        self.label_encoder = None
        self.num_labels = None
    
    def fit(self, training_data: TrainingData) -> None:
        """Train the RoBERTa classifier.
        
        Args:
            training_data: Training data containing texts and labels
        """
        self._validate_training_data(training_data)
        
        self.classification_type = training_data.classification_type
        
        # Preprocess texts
        processed_texts = []
        for text in training_data.texts:
            cleaned = clean_text(text)
            normalized = normalize_text(cleaned)
            preprocessed = self.preprocessor.preprocess_text(normalized)
            processed_texts.append(preprocessed if preprocessed else text)  # Fallback to original if empty
        
        # Prepare labels
        if self.classification_type == ClassificationType.MULTI_CLASS:
            self.label_encoder = LabelEncoder()
            encoded_labels = self.label_encoder.fit_transform(training_data.labels)
            self.classes_ = self.label_encoder.classes_.tolist()
            self.num_labels = len(self.classes_)
        else:
            # Multi-label classification
            self.label_encoder = MultiLabelBinarizer()
            encoded_labels = self.label_encoder.fit_transform(training_data.labels)
            self.classes_ = self.label_encoder.classes_.tolist()
            self.num_labels = len(self.classes_)
        
        # Initialize tokenizer and model
        try:
            self.tokenizer = RobertaTokenizer.from_pretrained(self.model_name)
            
            if self.classification_type == ClassificationType.MULTI_CLASS:
                self.model = RobertaForSequenceClassification.from_pretrained(
                    self.model_name,
                    num_labels=self.num_labels
                )
            else:
                # For multi-label, we need a custom model
                self.model = self._create_multilabel_model()
            
            self.model.to(self.device)
            
        except Exception as e:
            raise ModelTrainingError(f"Failed to initialize RoBERTa model: {str(e)}", self.config.model_name)
        
        # Create dataset and dataloader
        dataset = TextDataset(processed_texts, encoded_labels, self.tokenizer, self.max_length)
        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)
        
        # Setup optimizer and scheduler
        optimizer = AdamW(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)
        
        total_steps = len(dataloader) * self.num_epochs
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=self.warmup_steps,
            num_training_steps=total_steps
        )
        
        # Training loop
        self.model.train()
        
        for epoch in range(self.num_epochs):
            total_loss = 0
            
            for batch in dataloader:
                # Move batch to device
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                # Forward pass
                optimizer.zero_grad()
                
                if self.classification_type == ClassificationType.MULTI_CLASS:
                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                    loss = outputs.loss
                else:
                    # Multi-label classification
                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                    loss = self._compute_multilabel_loss(outputs.logits, labels.float())
                
                # Backward pass
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                
                optimizer.step()
                scheduler.step()
                
                total_loss += loss.item()
            
            avg_loss = total_loss / len(dataloader)
            print(f"Epoch {epoch + 1}/{self.num_epochs}, Average Loss: {avg_loss:.4f}")
        
        self.is_trained = True
    
    def predict(self, texts: List[str]) -> ClassificationResult:
        """Predict labels for texts.
        
        Args:
            texts: List of texts to classify
            
        Returns:
            ClassificationResult with predictions
        """
        self.validate_input(texts)
        
        if not self.is_trained:
            raise PredictionError("Model must be trained before prediction", self.config.model_name)
        
        # Preprocess texts
        processed_texts = []
        for text in texts:
            cleaned = clean_text(text)
            normalized = normalize_text(cleaned)
            preprocessed = self.preprocessor.preprocess_text(normalized)
            processed_texts.append(preprocessed if preprocessed else text)
        
        # Create dataset and dataloader
        dummy_labels = [0] * len(processed_texts)  # Dummy labels for prediction
        dataset = TextDataset(processed_texts, dummy_labels, self.tokenizer, self.max_length)
        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)
        
        # Prediction
        self.model.eval()
        all_predictions = []
        
        with torch.no_grad():
            for batch in dataloader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                logits = outputs.logits
                
                if self.classification_type == ClassificationType.MULTI_CLASS:
                    predictions = torch.argmax(logits, dim=-1)
                    batch_predictions = self.label_encoder.inverse_transform(predictions.cpu().numpy())
                    all_predictions.extend(batch_predictions)
                else:
                    # Multi-label classification
                    probabilities = torch.sigmoid(logits)
                    threshold = self.config.parameters.get('threshold', 0.5)
                    predictions = (probabilities > threshold).cpu().numpy()
                    batch_predictions = self.label_encoder.inverse_transform(predictions)
                    all_predictions.extend(batch_predictions.tolist())
        
        return self._create_result(predictions=all_predictions)
    
    def predict_proba(self, texts: List[str]) -> ClassificationResult:
        """Predict class probabilities for texts.
        
        Args:
            texts: List of texts to classify
            
        Returns:
            ClassificationResult with predictions and probabilities
        """
        self.validate_input(texts)
        
        if not self.is_trained:
            raise PredictionError("Model must be trained before prediction", self.config.model_name)
        
        # Preprocess texts
        processed_texts = []
        for text in texts:
            cleaned = clean_text(text)
            normalized = normalize_text(cleaned)
            preprocessed = self.preprocessor.preprocess_text(normalized)
            processed_texts.append(preprocessed if preprocessed else text)
        
        # Create dataset and dataloader
        dummy_labels = [0] * len(processed_texts)
        dataset = TextDataset(processed_texts, dummy_labels, self.tokenizer, self.max_length)
        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)
        
        # Prediction with probabilities
        self.model.eval()
        all_predictions = []
        all_probabilities = []
        all_confidence_scores = []
        
        with torch.no_grad():
            for batch in dataloader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                logits = outputs.logits
                
                if self.classification_type == ClassificationType.MULTI_CLASS:
                    probabilities = torch.softmax(logits, dim=-1)
                    predictions = torch.argmax(probabilities, dim=-1)
                    
                    batch_predictions = self.label_encoder.inverse_transform(predictions.cpu().numpy())
                    batch_probabilities = probabilities.cpu().numpy()
                    
                    for i, pred in enumerate(batch_predictions):
                        all_predictions.append(pred)
                        
                        # Create probability dictionary
                        prob_dict = {
                            class_name: float(batch_probabilities[i][j])
                            for j, class_name in enumerate(self.classes_)
                        }
                        all_probabilities.append(prob_dict)
                        all_confidence_scores.append(float(batch_probabilities[i][predictions[i]]))
                
                else:
                    # Multi-label classification
                    probabilities = torch.sigmoid(logits)
                    threshold = self.config.parameters.get('threshold', 0.5)
                    predictions = (probabilities > threshold).cpu().numpy()
                    
                    batch_predictions = self.label_encoder.inverse_transform(predictions)
                    batch_probabilities = probabilities.cpu().numpy()
                    
                    for i, pred in enumerate(batch_predictions):
                        all_predictions.append(pred.tolist())
                        
                        # Create probability dictionary
                        prob_dict = {
                            class_name: float(batch_probabilities[i][j])
                            for j, class_name in enumerate(self.classes_)
                        }
                        all_probabilities.append(prob_dict)
                        
                        # Confidence is max probability for multi-label
                        all_confidence_scores.append(float(np.max(batch_probabilities[i])))
        
        return self._create_result(
            predictions=all_predictions,
            probabilities=all_probabilities,
            confidence_scores=all_confidence_scores
        )
    
    def _create_multilabel_model(self):
        """Create a custom model for multi-label classification."""
        class RobertaMultiLabel(nn.Module):
            def __init__(self, model_name, num_labels):
                super().__init__()
                self.roberta = RobertaModel.from_pretrained(model_name)
                self.dropout = nn.Dropout(0.1)
                self.classifier = nn.Linear(self.roberta.config.hidden_size, num_labels)
            
            def forward(self, input_ids, attention_mask):
                outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)
                pooled_output = outputs.pooler_output
                pooled_output = self.dropout(pooled_output)
                logits = self.classifier(pooled_output)
                return type('obj', (object,), {'logits': logits})()
        
        return RobertaMultiLabel(self.model_name, self.num_labels)
    
    def _compute_multilabel_loss(self, logits, labels):
        """Compute loss for multi-label classification."""
        return nn.BCEWithLogitsLoss()(logits, labels)
    
    @property
    def model_info(self) -> Dict[str, Any]:
        """Get RoBERTa model information."""
        info = super().model_info
        info.update({
            "provider": "huggingface",
            "model_name": self.model_name,
            "max_length": self.max_length,
            "batch_size": self.batch_size,
            "learning_rate": self.learning_rate,
            "num_epochs": self.num_epochs,
            "device": str(self.device),
            "num_labels": self.num_labels
        })
        return info



# ./textclassify/prompt_engineer/base.py
class prompt_engineer:
    """
    Base class for prompt engineering.
    This class provides a template for creating prompt engineers.
    """

    def __init__(self, model_name: str):
        self.model_name = model_name

    def generate_prompt_single_label(self, input_data: str) -> str:
        """
        Generate a prompt based on the input data.
        This method should be overridden by subclasses.
        """
        raise NotImplementedError("Subclasses should implement this method.")
    
    def generate_prompt_multiple_labels(self, input_data: str) -> str:
        """
        Generate a prompt for multiple labels based on the input data.
        This method should be overridden by subclasses.
        """
        raise NotImplementedError("Subclasses should implement this method.")
    
    
    


# ./textclassify/prompt_engineer/prompt_warehouse.py


# ./textclassify/utils/__init__.py
"""Utility functions and helpers."""

from .logging import setup_logging, get_logger
from .metrics import ClassificationMetrics, evaluate_predictions
from .data import DataLoader, split_data, balance_data

__all__ = [
    "setup_logging",
    "get_logger",
    "ClassificationMetrics",
    "evaluate_predictions",
    "DataLoader",
    "split_data",
    "balance_data",
]



# ./textclassify/utils/data.py
"""Data handling utilities for text classification."""

import csv
import json
import random
from pathlib import Path
from typing import Dict, List, Tuple, Union, Optional, Any
from collections import Counter, defaultdict

from ..core.types import TrainingData, ClassificationType


class DataLoader:
    """Utility class for loading and handling text classification data."""
    
    @staticmethod
    def from_csv(
        file_path: str,
        text_column: str = 'text',
        label_column: str = 'label',
        classification_type: ClassificationType = ClassificationType.MULTI_CLASS,
        delimiter: str = ',',
        encoding: str = 'utf-8'
    ) -> TrainingData:
        """Load data from CSV file.
        
        Args:
            file_path: Path to CSV file
            text_column: Name of text column
            label_column: Name of label column
            classification_type: Type of classification
            delimiter: CSV delimiter
            encoding: File encoding
            
        Returns:
            TrainingData instance
        """
        texts = []
        labels = []
        
        with open(file_path, 'r', encoding=encoding) as f:
            reader = csv.DictReader(f, delimiter=delimiter)
            
            for row in reader:
                if text_column not in row or label_column not in row:
                    continue
                
                text = row[text_column].strip()
                if not text:
                    continue
                
                if classification_type == ClassificationType.MULTI_CLASS:
                    label = row[label_column].strip()
                    labels.append(label)
                else:
                    # Multi-label: assume labels are separated by semicolons or commas
                    label_str = row[label_column].strip()
                    if ';' in label_str:
                        label_list = [l.strip() for l in label_str.split(';') if l.strip()]
                    else:
                        label_list = [l.strip() for l in label_str.split(',') if l.strip()]
                    labels.append(label_list)
                
                texts.append(text)
        
        return TrainingData(
            texts=texts,
            labels=labels,
            classification_type=classification_type
        )
    
    @staticmethod
    def from_json(
        file_path: str,
        text_field: str = 'text',
        label_field: str = 'label',
        classification_type: ClassificationType = ClassificationType.MULTI_CLASS,
        encoding: str = 'utf-8'
    ) -> TrainingData:
        """Load data from JSON file.
        
        Args:
            file_path: Path to JSON file
            text_field: Name of text field
            label_field: Name of label field
            classification_type: Type of classification
            encoding: File encoding
            
        Returns:
            TrainingData instance
        """
        with open(file_path, 'r', encoding=encoding) as f:
            data = json.load(f)
        
        if not isinstance(data, list):
            raise ValueError("JSON file must contain a list of objects")
        
        texts = []
        labels = []
        
        for item in data:
            if not isinstance(item, dict):
                continue
            
            if text_field not in item or label_field not in item:
                continue
            
            text = str(item[text_field]).strip()
            if not text:
                continue
            
            if classification_type == ClassificationType.MULTI_CLASS:
                label = str(item[label_field]).strip()
                labels.append(label)
            else:
                # Multi-label: expect list or string
                label_data = item[label_field]
                if isinstance(label_data, list):
                    label_list = [str(l).strip() for l in label_data if str(l).strip()]
                else:
                    label_str = str(label_data).strip()
                    if ';' in label_str:
                        label_list = [l.strip() for l in label_str.split(';') if l.strip()]
                    else:
                        label_list = [l.strip() for l in label_str.split(',') if l.strip()]
                labels.append(label_list)
            
            texts.append(text)
        
        return TrainingData(
            texts=texts,
            labels=labels,
            classification_type=classification_type
        )
    
    @staticmethod
    def from_lists(
        texts: List[str],
        labels: Union[List[str], List[List[str]]],
        classification_type: ClassificationType
    ) -> TrainingData:
        """Create TrainingData from lists.
        
        Args:
            texts: List of texts
            labels: List of labels
            classification_type: Type of classification
            
        Returns:
            TrainingData instance
        """
        return TrainingData(
            texts=texts,
            labels=labels,
            classification_type=classification_type
        )
    
    @staticmethod
    def save_to_csv(
        training_data: TrainingData,
        file_path: str,
        text_column: str = 'text',
        label_column: str = 'label',
        delimiter: str = ',',
        encoding: str = 'utf-8'
    ) -> None:
        """Save training data to CSV file.
        
        Args:
            training_data: Training data to save
            file_path: Path to save CSV file
            text_column: Name of text column
            label_column: Name of label column
            delimiter: CSV delimiter
            encoding: File encoding
        """
        with open(file_path, 'w', newline='', encoding=encoding) as f:
            writer = csv.writer(f, delimiter=delimiter)
            
            # Write header
            writer.writerow([text_column, label_column])
            
            # Write data
            for text, label in zip(training_data.texts, training_data.labels):
                if training_data.classification_type == ClassificationType.MULTI_CLASS:
                    writer.writerow([text, label])
                else:
                    # Multi-label: join with semicolons
                    label_str = ';'.join(label) if isinstance(label, list) else str(label)
                    writer.writerow([text, label_str])
    
    @staticmethod
    def save_to_json(
        training_data: TrainingData,
        file_path: str,
        text_field: str = 'text',
        label_field: str = 'label',
        encoding: str = 'utf-8'
    ) -> None:
        """Save training data to JSON file.
        
        Args:
            training_data: Training data to save
            file_path: Path to save JSON file
            text_field: Name of text field
            label_field: Name of label field
            encoding: File encoding
        """
        data = []
        
        for text, label in zip(training_data.texts, training_data.labels):
            item = {
                text_field: text,
                label_field: label
            }
            data.append(item)
        
        with open(file_path, 'w', encoding=encoding) as f:
            json.dump(data, f, indent=2, ensure_ascii=False)


def split_data(
    training_data: TrainingData,
    train_ratio: float = 0.8,
    random_seed: Optional[int] = None,
    stratify: bool = True
) -> Tuple[TrainingData, TrainingData]:
    """Split training data into train and validation sets.
    
    Args:
        training_data: Original training data
        train_ratio: Ratio of data to use for training
        random_seed: Random seed for reproducibility
        stratify: Whether to stratify split by labels
        
    Returns:
        Tuple of (train_data, val_data)
    """
    if random_seed is not None:
        random.seed(random_seed)
    
    indices = list(range(len(training_data.texts)))
    
    if stratify and training_data.classification_type == ClassificationType.MULTI_CLASS:
        # Stratified split for multi-class
        label_indices = defaultdict(list)
        for i, label in enumerate(training_data.labels):
            label_indices[label].append(i)
        
        train_indices = []
        val_indices = []
        
        for label, label_idx_list in label_indices.items():
            random.shuffle(label_idx_list)
            split_point = int(len(label_idx_list) * train_ratio)
            train_indices.extend(label_idx_list[:split_point])
            val_indices.extend(label_idx_list[split_point:])
    
    else:
        # Random split
        random.shuffle(indices)
        split_point = int(len(indices) * train_ratio)
        train_indices = indices[:split_point]
        val_indices = indices[split_point:]
    
    # Create training data splits
    train_texts = [training_data.texts[i] for i in train_indices]
    train_labels = [training_data.labels[i] for i in train_indices]
    
    val_texts = [training_data.texts[i] for i in val_indices]
    val_labels = [training_data.labels[i] for i in val_indices]
    
    train_data = TrainingData(
        texts=train_texts,
        labels=train_labels,
        classification_type=training_data.classification_type
    )
    
    val_data = TrainingData(
        texts=val_texts,
        labels=val_labels,
        classification_type=training_data.classification_type
    )
    
    return train_data, val_data


def balance_data(
    training_data: TrainingData,
    method: str = 'oversample',
    random_seed: Optional[int] = None
) -> TrainingData:
    """Balance training data by class distribution.
    
    Args:
        training_data: Original training data
        method: Balancing method ('oversample', 'undersample')
        random_seed: Random seed for reproducibility
        
    Returns:
        Balanced training data
    """
    if training_data.classification_type != ClassificationType.MULTI_CLASS:
        raise ValueError("Balancing is only supported for multi-class classification")
    
    if random_seed is not None:
        random.seed(random_seed)
    
    # Count samples per class
    label_counts = Counter(training_data.labels)
    
    if method == 'oversample':
        # Oversample to match the largest class
        max_count = max(label_counts.values())
        target_count = max_count
    elif method == 'undersample':
        # Undersample to match the smallest class
        min_count = min(label_counts.values())
        target_count = min_count
    else:
        raise ValueError("Method must be 'oversample' or 'undersample'")
    
    # Group samples by label
    label_samples = defaultdict(list)
    for i, label in enumerate(training_data.labels):
        label_samples[label].append(i)
    
    # Balance each class
    balanced_indices = []
    
    for label, sample_indices in label_samples.items():
        current_count = len(sample_indices)
        
        if method == 'oversample' and current_count < target_count:
            # Oversample by repeating samples
            needed = target_count - current_count
            additional_indices = random.choices(sample_indices, k=needed)
            balanced_indices.extend(sample_indices + additional_indices)
        
        elif method == 'undersample' and current_count > target_count:
            # Undersample by randomly selecting samples
            selected_indices = random.sample(sample_indices, target_count)
            balanced_indices.extend(selected_indices)
        
        else:
            # Keep all samples
            balanced_indices.extend(sample_indices)
    
    # Shuffle the balanced indices
    random.shuffle(balanced_indices)
    
    # Create balanced training data
    balanced_texts = [training_data.texts[i] for i in balanced_indices]
    balanced_labels = [training_data.labels[i] for i in balanced_indices]
    
    return TrainingData(
        texts=balanced_texts,
        labels=balanced_labels,
        classification_type=training_data.classification_type
    )


def get_data_statistics(training_data: TrainingData) -> Dict[str, Any]:
    """Get statistics about the training data.
    
    Args:
        training_data: Training data to analyze
        
    Returns:
        Dictionary containing data statistics
    """
    stats = {
        'total_samples': len(training_data.texts),
        'classification_type': training_data.classification_type.value,
        'text_lengths': {
            'min': min(len(text) for text in training_data.texts),
            'max': max(len(text) for text in training_data.texts),
            'mean': sum(len(text) for text in training_data.texts) / len(training_data.texts),
        }
    }
    
    if training_data.classification_type == ClassificationType.MULTI_CLASS:
        # Multi-class statistics
        label_counts = Counter(training_data.labels)
        stats['num_classes'] = len(label_counts)
        stats['class_distribution'] = dict(label_counts)
        stats['class_balance'] = {
            'most_common': label_counts.most_common(1)[0],
            'least_common': label_counts.most_common()[-1],
            'balance_ratio': label_counts.most_common()[-1][1] / label_counts.most_common(1)[0][1]
        }
    
    else:
        # Multi-label statistics
        all_labels = []
        for label_list in training_data.labels:
            all_labels.extend(label_list)
        
        label_counts = Counter(all_labels)
        labels_per_sample = [len(label_list) for label_list in training_data.labels]
        
        stats['num_classes'] = len(label_counts)
        stats['class_distribution'] = dict(label_counts)
        stats['labels_per_sample'] = {
            'min': min(labels_per_sample),
            'max': max(labels_per_sample),
            'mean': sum(labels_per_sample) / len(labels_per_sample)
        }
    
    return stats



# ./textclassify/utils/logging.py
"""Logging utilities for textclassify package."""

import logging
import sys
from pathlib import Path
from typing import Optional


def setup_logging(
    level: str = "INFO",
    log_file: Optional[str] = None,
    format_string: Optional[str] = None
) -> logging.Logger:
    """Set up logging for the textclassify package.
    
    Args:
        level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
        log_file: Optional path to log file
        format_string: Optional custom format string
        
    Returns:
        Configured logger instance
    """
    # Default format
    if format_string is None:
        format_string = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    
    # Convert level string to logging constant
    numeric_level = getattr(logging, level.upper(), logging.INFO)
    
    # Create formatter
    formatter = logging.Formatter(format_string)
    
    # Get root logger for textclassify
    logger = logging.getLogger("textclassify")
    logger.setLevel(numeric_level)
    
    # Remove existing handlers to avoid duplicates
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(numeric_level)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # File handler (optional)
    if log_file:
        log_path = Path(log_file).expanduser()
        log_path.parent.mkdir(parents=True, exist_ok=True)
        
        file_handler = logging.FileHandler(log_path)
        file_handler.setLevel(numeric_level)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    # Prevent propagation to root logger
    logger.propagate = False
    
    return logger


def get_logger(name: str) -> logging.Logger:
    """Get a logger instance for a specific module.
    
    Args:
        name: Logger name (usually __name__)
        
    Returns:
        Logger instance
    """
    return logging.getLogger(f"textclassify.{name}")


class LoggerMixin:
    """Mixin class to add logging capabilities to other classes."""
    
    @property
    def logger(self) -> logging.Logger:
        """Get logger for this class."""
        class_name = self.__class__.__name__
        module_name = self.__class__.__module__
        
        # Extract module name after textclassify
        if "textclassify" in module_name:
            parts = module_name.split("textclassify.")
            if len(parts) > 1:
                logger_name = f"textclassify.{parts[1]}.{class_name}"
            else:
                logger_name = f"textclassify.{class_name}"
        else:
            logger_name = f"textclassify.{class_name}"
        
        return logging.getLogger(logger_name)


def log_function_call(func):
    """Decorator to log function calls.
    
    Args:
        func: Function to decorate
        
    Returns:
        Decorated function
    """
    def wrapper(*args, **kwargs):
        logger = get_logger(func.__module__)
        logger.debug(f"Calling {func.__name__} with args={args}, kwargs={kwargs}")
        
        try:
            result = func(*args, **kwargs)
            logger.debug(f"{func.__name__} completed successfully")
            return result
        except Exception as e:
            logger.error(f"{func.__name__} failed with error: {str(e)}")
            raise
    
    return wrapper


def log_performance(func):
    """Decorator to log function performance.
    
    Args:
        func: Function to decorate
        
    Returns:
        Decorated function
    """
    import time
    
    def wrapper(*args, **kwargs):
        logger = get_logger(func.__module__)
        start_time = time.time()
        
        try:
            result = func(*args, **kwargs)
            end_time = time.time()
            duration = end_time - start_time
            logger.info(f"{func.__name__} completed in {duration:.2f} seconds")
            return result
        except Exception as e:
            end_time = time.time()
            duration = end_time - start_time
            logger.error(f"{func.__name__} failed after {duration:.2f} seconds: {str(e)}")
            raise
    
    return wrapper



# ./textclassify/utils/metrics.py
"""Evaluation metrics for text classification."""

import numpy as np
from typing import Dict, List, Union, Optional, Any
from collections import defaultdict

from ..core.types import ClassificationResult, ClassificationType


class ClassificationMetrics:
    """Comprehensive metrics for text classification evaluation."""
    
    def __init__(self, classification_type: ClassificationType):
        """Initialize metrics calculator.
        
        Args:
            classification_type: Type of classification task
        """
        self.classification_type = classification_type
    
    def calculate_metrics(
        self,
        y_true: List[Union[str, List[str]]],
        y_pred: List[Union[str, List[str]]],
        classes: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """Calculate comprehensive metrics.
        
        Args:
            y_true: True labels
            y_pred: Predicted labels
            classes: List of class names (optional)
            
        Returns:
            Dictionary containing various metrics
        """
        if len(y_true) != len(y_pred):
            raise ValueError("y_true and y_pred must have the same length")
        
        if self.classification_type == ClassificationType.MULTI_CLASS:
            return self._calculate_multiclass_metrics(y_true, y_pred, classes)
        else:
            return self._calculate_multilabel_metrics(y_true, y_pred, classes)
    
    def _calculate_multiclass_metrics(
        self,
        y_true: List[str],
        y_pred: List[str],
        classes: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """Calculate metrics for multi-class classification."""
        
        # Get unique classes
        if classes is None:
            classes = sorted(list(set(y_true + y_pred)))
        
        # Calculate basic metrics
        correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)
        total = len(y_true)
        accuracy = correct / total if total > 0 else 0.0
        
        # Per-class metrics
        class_metrics = {}
        confusion_matrix = self._build_confusion_matrix(y_true, y_pred, classes)
        
        macro_precision = 0.0
        macro_recall = 0.0
        macro_f1 = 0.0
        
        for i, class_name in enumerate(classes):
            tp = confusion_matrix[i][i]
            fp = sum(confusion_matrix[j][i] for j in range(len(classes)) if j != i)
            fn = sum(confusion_matrix[i][j] for j in range(len(classes)) if j != i)
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
            
            class_metrics[class_name] = {
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'support': sum(1 for label in y_true if label == class_name)
            }
            
            macro_precision += precision
            macro_recall += recall
            macro_f1 += f1
        
        # Macro averages
        num_classes = len(classes)
        macro_precision /= num_classes
        macro_recall /= num_classes
        macro_f1 /= num_classes
        
        # Weighted averages
        total_support = sum(class_metrics[cls]['support'] for cls in classes)
        weighted_precision = sum(
            class_metrics[cls]['precision'] * class_metrics[cls]['support']
            for cls in classes
        ) / total_support if total_support > 0 else 0.0
        
        weighted_recall = sum(
            class_metrics[cls]['recall'] * class_metrics[cls]['support']
            for cls in classes
        ) / total_support if total_support > 0 else 0.0
        
        weighted_f1 = sum(
            class_metrics[cls]['f1_score'] * class_metrics[cls]['support']
            for cls in classes
        ) / total_support if total_support > 0 else 0.0
        
        return {
            'accuracy': accuracy,
            'macro_precision': macro_precision,
            'macro_recall': macro_recall,
            'macro_f1': macro_f1,
            'weighted_precision': weighted_precision,
            'weighted_recall': weighted_recall,
            'weighted_f1': weighted_f1,
            'per_class_metrics': class_metrics,
            'confusion_matrix': confusion_matrix,
            'classes': classes,
            'total_samples': total
        }
    
    def _calculate_multilabel_metrics(
        self,
        y_true: List[List[str]],
        y_pred: List[List[str]],
        classes: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """Calculate metrics for multi-label classification."""
        
        # Get unique classes
        if classes is None:
            all_labels = set()
            for labels in y_true + y_pred:
                all_labels.update(labels)
            classes = sorted(list(all_labels))
        
        # Convert to binary format
        y_true_binary = self._to_binary_matrix(y_true, classes)
        y_pred_binary = self._to_binary_matrix(y_pred, classes)
        
        # Calculate sample-wise metrics
        exact_match = sum(
            1 for true_labels, pred_labels in zip(y_true, y_pred)
            if set(true_labels) == set(pred_labels)
        ) / len(y_true)
        
        # Hamming loss (fraction of wrong labels)
        hamming_loss = np.mean(y_true_binary != y_pred_binary)
        
        # Per-class metrics
        class_metrics = {}
        macro_precision = 0.0
        macro_recall = 0.0
        macro_f1 = 0.0
        
        for i, class_name in enumerate(classes):
            tp = np.sum((y_true_binary[:, i] == 1) & (y_pred_binary[:, i] == 1))
            fp = np.sum((y_true_binary[:, i] == 0) & (y_pred_binary[:, i] == 1))
            fn = np.sum((y_true_binary[:, i] == 1) & (y_pred_binary[:, i] == 0))
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
            
            class_metrics[class_name] = {
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'support': np.sum(y_true_binary[:, i])
            }
            
            macro_precision += precision
            macro_recall += recall
            macro_f1 += f1
        
        # Macro averages
        num_classes = len(classes)
        macro_precision /= num_classes
        macro_recall /= num_classes
        macro_f1 /= num_classes
        
        # Micro averages (aggregate across all classes)
        total_tp = sum(
            np.sum((y_true_binary[:, i] == 1) & (y_pred_binary[:, i] == 1))
            for i in range(len(classes))
        )
        total_fp = sum(
            np.sum((y_true_binary[:, i] == 0) & (y_pred_binary[:, i] == 1))
            for i in range(len(classes))
        )
        total_fn = sum(
            np.sum((y_true_binary[:, i] == 1) & (y_pred_binary[:, i] == 0))
            for i in range(len(classes))
        )
        
        micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
        micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0
        micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0.0
        
        return {
            'exact_match_ratio': exact_match,
            'hamming_loss': hamming_loss,
            'macro_precision': macro_precision,
            'macro_recall': macro_recall,
            'macro_f1': macro_f1,
            'micro_precision': micro_precision,
            'micro_recall': micro_recall,
            'micro_f1': micro_f1,
            'per_class_metrics': class_metrics,
            'classes': classes,
            'total_samples': len(y_true)
        }
    
    def _build_confusion_matrix(
        self,
        y_true: List[str],
        y_pred: List[str],
        classes: List[str]
    ) -> List[List[int]]:
        """Build confusion matrix for multi-class classification."""
        
        class_to_idx = {cls: i for i, cls in enumerate(classes)}
        matrix = [[0] * len(classes) for _ in range(len(classes))]
        
        for true_label, pred_label in zip(y_true, y_pred):
            if true_label in class_to_idx and pred_label in class_to_idx:
                true_idx = class_to_idx[true_label]
                pred_idx = class_to_idx[pred_label]
                matrix[true_idx][pred_idx] += 1
        
        return matrix
    
    def _to_binary_matrix(
        self,
        labels: List[List[str]],
        classes: List[str]
    ) -> np.ndarray:
        """Convert multi-label format to binary matrix."""
        
        class_to_idx = {cls: i for i, cls in enumerate(classes)}
        matrix = np.zeros((len(labels), len(classes)), dtype=int)
        
        for i, label_list in enumerate(labels):
            for label in label_list:
                if label in class_to_idx:
                    matrix[i, class_to_idx[label]] = 1
        
        return matrix


def evaluate_predictions(
    result: ClassificationResult,
    y_true: List[Union[str, List[str]]],
    classes: Optional[List[str]] = None
) -> Dict[str, Any]:
    """Evaluate classification predictions.
    
    Args:
        result: Classification result from a model
        y_true: True labels
        classes: List of class names (optional)
        
    Returns:
        Dictionary containing evaluation metrics
    """
    if result.classification_type is None:
        raise ValueError("Classification type must be specified in result")
    
    metrics_calculator = ClassificationMetrics(result.classification_type)
    metrics = metrics_calculator.calculate_metrics(y_true, result.predictions, classes)
    
    # Add model information
    metrics['model_info'] = {
        'model_name': result.model_name,
        'model_type': result.model_type.value if result.model_type else None,
        'processing_time': result.processing_time
    }
    
    return metrics


def compare_models(
    results: List[ClassificationResult],
    y_true: List[Union[str, List[str]]],
    model_names: Optional[List[str]] = None,
    classes: Optional[List[str]] = None
) -> Dict[str, Any]:
    """Compare multiple model results.
    
    Args:
        results: List of classification results
        y_true: True labels
        model_names: Optional list of model names
        classes: List of class names (optional)
        
    Returns:
        Dictionary containing comparison metrics
    """
    if not results:
        raise ValueError("At least one result is required")
    
    if model_names and len(model_names) != len(results):
        raise ValueError("Number of model names must match number of results")
    
    comparison = {
        'models': {},
        'summary': {}
    }
    
    # Evaluate each model
    for i, result in enumerate(results):
        model_name = model_names[i] if model_names else f"model_{i+1}"
        metrics = evaluate_predictions(result, y_true, classes)
        comparison['models'][model_name] = metrics
    
    # Create summary comparison
    if results[0].classification_type == ClassificationType.MULTI_CLASS:
        key_metrics = ['accuracy', 'macro_f1', 'weighted_f1']
    else:
        key_metrics = ['exact_match_ratio', 'macro_f1', 'micro_f1']
    
    for metric in key_metrics:
        comparison['summary'][metric] = {
            model_name: comparison['models'][model_name][metric]
            for model_name in comparison['models']
        }
    
    # Find best model for each metric
    comparison['best_models'] = {}
    for metric in key_metrics:
        best_model = max(
            comparison['summary'][metric],
            key=comparison['summary'][metric].get
        )
        comparison['best_models'][metric] = best_model
    
    return comparison



