@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{liu2019roberta,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{pedregosa2011scikit,
  title={Scikit-learn: Machine Learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={Oct},
  pages={2825--2830},
  year={2011}
}

@article{wolf2019huggingface,
  title={HuggingFace's Transformers: State-of-the-Art Natural Language Processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

@inproceedings{guo2017calibration,
  title={On Calibration of Modern Neural Networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={Proceedings of the 34th International Conference on Machine Learning},
  year={2017}
}

@inproceedings{zadrozny2002transforming,
  title={Transforming Classifier Scores into Accurate Multiclass Probability Estimates},
  author={Zadrozny, Bianca and Elkan, Charles},
  booktitle={Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={694--699},
  year={2002}
}

@inproceedings{paszke2019pytorch,
  title={PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{zhang2015character,
  title={Character-level Convolutional Networks for Text Classification},
  author={Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  booktitle={Advances in Neural Information Processing Systems},
  volume={28},
  pages={649--657},
  year={2015}
}

@inproceedings{demszky2020goemotions,
  title={GoEmotions: A Dataset of Fine-Grained Emotions},
  author={Demszky, Dorottya and Movshovitz-Attias, Dana and Ko, Jeongwoo and Cowen, Alan and Nemade, Gaurav and Ravi, Sujith},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={4040--4054},
  year={2020}
}

@article{thielmann2021unsupervised,
author = {Anton Thielmann and Christoph Weisser and Astrid Krenz and Benjamin Säfken},
title = {Unsupervised document classification integrating web scraping, one-class SVM and LDA topic modelling},
journal = {Journal of Applied Statistics},
volume = {50},
number = {3},
pages = {574--591},
year = {2021},
publisher = {Taylor \& Francis},
doi = {10.1080/02664763.2021.1919063},
note ={PMID: 36819086},
URL = {https://doi.org/10.1080/02664763.2021.1919063
},
eprint = {https://doi.org/10.1080/02664763.2021.1919063
}
}

@Inbook{thielmann2021one,
author="Thielmann, Anton
and Weisser, Christoph
and Krenz, Astrid",
editor="Phuong, Nguyen Hoang
and Kreinovich, Vladik",
title="One-Class Support Vector Machine and LDA Topic Model Integration---Evidence for AI Patents",
bookTitle="Soft Computing: Biomedical and Related Applications",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="263--272",
abstract="The present contribution suggests a two-step classification rule for unsupervised document classification, using one-class Support Vector Machines and Latent Dirichlet Allocation Topic Modeling. The integration of both algorithms allows the usage of labelled, but independent training data, not stemming from the data set to be classified. The manual labelling when trying to classify a specific class from an unlabelled data set can thus be circumvented. By choosing appropriate document representations and parameters in the one-class Support Vector Machine, the differences between the independent training class and the data set to be classified become negligible. The method is applied to a large data set on patents for the European Union.",
isbn="978-3-030-76620-7",
doi="10.1007/978-3-030-76620-7_23",
url="https://doi.org/10.1007/978-3-030-76620-7_23"
}

@inproceedings{thielmann2024human,
    title = "Human in the Loop: How to Effectively Create Coherent Topics by Manually Labeling Only a Few Documents per Class",
    author = {Thielmann, Anton F.  and
      Weisser, Christoph  and
      S{\"a}fken, Benjamin},
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.736/",
    pages = "8395--8405",
    abstract = "Few-shot methods for accurate modeling under sparse label-settings have improved significantly. However, the applications of few-shot modeling in natural language processing remain solely in the field of document classification. With recent performance improvements, supervised few-shot methods, combined with a simple topic extraction method pose a significant challenge to unsupervised topic modeling methods. Our research shows that supervised few-shot learning, combined with a simple topic extraction method, can outperform unsupervised topic modeling techniques in terms of generating coherent topics, even when only a few labeled documents per class are used. The code is available at the following link: https://github.com/AnFreTh/STREAM"
}

@article{kant2025iteative,
  author    = {Gillian Kant and Levin Wiebelt and Christoph Weisser and Krisztina Kis-Katos and Mattias Luber and Benjamin S{\"a}fken},
  title     = {An iterative topic model filtering framework for short and noisy user-generated data: analyzing conspiracy theories on twitter},
  journal   = {International Journal of Data Science and Analytics},
  year      = {2025},
  volume    = {20},
  number    = {2},
  pages     = {269--289},
  doi       = {10.1007/s41060-022-00321-4},
  url       = {https://doi.org/10.1007/s41060-022-00321-4},
  issn      = {2364-4168},
  abstract  = {Conspiracy theories have seen a rise in popularity in recent years. Spreading quickly through social media, their disruptive effect can lead to a biased public view on policy decisions and events. We present a novel approach for LDA-pre-processing called Iterative Filtering to study such phenomena based on Twitter data. In combination with Hashtag Pooling as an additional pre-processing step, we are able to achieve a coherent framing of the discussion and topics of interest, despite of the inherent noisiness and sparseness of Twitter data. Our novel approach enables researchers to gain detailed insights into discourses of interest on Twitter, allowing them to identify tweets iteratively that are related to an investigated topic of interest. As an application, we study the dynamics of conspiracy-related topics on US Twitter during the last four months of 2020, which were dominated by the US-Presidential Elections and Covid-19. We monitor the public discourse in the USA with geo-spatial Twitter data to identify conspiracy-related contents by estimating Latent Dirichlet Allocation (LDA) Topic Models. We find that in this period, usual conspiracy-related topics played a marginal role in comparison with dominating topics, such as the US-Presidential Elections or the general discussions about Covid-19. The main conspiracy theories in this period were the ones linked to “Election Fraud” and the “Covid-19-hoax.” Conspiracy-related keywords tended to appear together with Trump-related words and words related to his presidential campaign.}
}


