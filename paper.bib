@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018},
  doi={10.48550/arXiv.1810.04805},
  url={https://doi.org/10.48550/arXiv.1810.04805}
}

@article{liu2019roberta,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019},
  doi={10.48550/arXiv.1907.11692},
  url={https://doi.org/10.48550/arXiv.1907.11692}
}

@article{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023},
  doi={10.48550/arXiv.2303.08774},
  url={https://doi.org/10.48550/arXiv.2303.08774}
}

@article{pedregosa2011scikit,
  title={Scikit-learn: Machine Learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={Oct},
  pages={2825--2830},
  year={2011},
  url={http://www.jmlr.org/papers/v12/pedregosa11a.html}
}

@article{wolf2019huggingface,
  title={HuggingFace's Transformers: State-of-the-Art Natural Language Processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019},
  doi={10.48550/arXiv.1910.03771},
  url={https://doi.org/10.48550/arXiv.1910.03771}
}

@inproceedings{guo2017calibration,
  title={On Calibration of Modern Neural Networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={Proceedings of the 34th International Conference on Machine Learning},
  series={Proceedings of Machine Learning Research},
  volume={70},
  pages={1321--1330},
  publisher={PMLR},
  year={2017},
  url={https://proceedings.mlr.press/v70/guo17a.html}
}

@inproceedings{zadrozny2002transforming,
  title={Transforming Classifier Scores into Accurate Multiclass Probability Estimates},
  author={Zadrozny, Bianca and Elkan, Charles},
  booktitle={Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={694--699},
  year={2002},
  publisher={Association for Computing Machinery},
  doi={10.1145/775047.775151}
}

@inproceedings{paszke2019pytorch,
  title={PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019},
  doi={10.48550/arXiv.1912.01703},
  url={https://doi.org/10.48550/arXiv.1912.01703}
}

@inproceedings{zhang2015character,
  title={Character-level Convolutional Networks for Text Classification},
  author={Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  booktitle={Advances in Neural Information Processing Systems},
  volume={28},
  pages={649--657},
  year={2015},
  doi={10.48550/arXiv.1509.01626},
  url={https://doi.org/10.48550/arXiv.1509.01626}
}

@inproceedings{lewis1997reuters,
  title={Reuters-21578 Text Categorization Test Collection, Distribution 1.0},
  author={Lewis, David D},
  booktitle={KDD Workshop on Text Mining},
  year={1997},
  url={https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html}
}


@article{thielmann2021unsupervised,
author = {Anton Thielmann and Christoph Weisser and Astrid Krenz and Benjamin Säfken},
title = {Unsupervised document classification integrating web scraping, one-class SVM and LDA topic modelling},
journal = {Journal of Applied Statistics},
volume = {50},
number = {3},
pages = {574--591},
year = {2021},
publisher = {Taylor \& Francis},
doi = {10.1080/02664763.2021.1919063},
note ={PMID: 36819086},
URL = {https://doi.org/10.1080/02664763.2021.1919063},
eprint = {https://doi.org/10.1080/02664763.2021.1919063
}
}

@Inbook{thielmann2021one,
author="Thielmann, Anton
and Weisser, Christoph
and Krenz, Astrid",
editor="Phuong, Nguyen Hoang
and Kreinovich, Vladik",
title="One-Class Support Vector Machine and LDA Topic Model Integration---Evidence for AI Patents",
bookTitle="Soft Computing: Biomedical and Related Applications",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="263--272",
abstract="The present contribution suggests a two-step classification rule for unsupervised document classification, using one-class Support Vector Machines and Latent Dirichlet Allocation Topic Modeling. The integration of both algorithms allows the usage of labelled, but independent training data, not stemming from the data set to be classified. The manual labelling when trying to classify a specific class from an unlabelled data set can thus be circumvented. By choosing appropriate document representations and parameters in the one-class Support Vector Machine, the differences between the independent training class and the data set to be classified become negligible. The method is applied to a large data set on patents for the European Union.",
isbn="978-3-030-76620-7",
doi="10.1007/978-3-030-76620-7_23",
url={https://doi.org/10.1007/978-3-030-76620-7_23"}
}

@inproceedings{thielmann2024human,
    title = "Human in the Loop: How to Effectively Create Coherent Topics by Manually Labeling Only a Few Documents per Class",
    author = {Thielmann, Anton F.  and
      Weisser, Christoph  and
      S{\"a}fken, Benjamin},
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.736/",
    doi = "https://doi.org/10.48550/arXiv.2212.09422",
    pages = "8395--8405",
    abstract = "Few-shot methods for accurate modeling under sparse label-settings have improved significantly. However, the applications of few-shot modeling in natural language processing remain solely in the field of document classification. With recent performance improvements, supervised few-shot methods, combined with a simple topic extraction method pose a significant challenge to unsupervised topic modeling methods. Our research shows that supervised few-shot learning, combined with a simple topic extraction method, can outperform unsupervised topic modeling techniques in terms of generating coherent topics, even when only a few labeled documents per class are used. The code is available at the following link: https://github.com/AnFreTh/STREAM"
}

@article{kant2022iteative,
  author    = {Gillian Kant and Levin Wiebelt and Christoph Weisser and Krisztina Kis-Katos and Mattias Luber and Benjamin S{\"a}fken},
  title     = {An iterative topic model filtering framework for short and noisy user-generated data: analyzing conspiracy theories on twitter},
  journal   = {International Journal of Data Science and Analytics},
  year      = {2022},
  volume    = {20},
  number    = {2},
  pages     = {269--289},
  doi       = {10.1007/s41060-022-00321-4},
  url       = {https://doi.org/10.1007/s41060-022-00321-4},
  issn      = {2364-4168},
  abstract  = {Conspiracy theories have seen a rise in popularity in recent years. Spreading quickly through social media, their disruptive effect can lead to a biased public view on policy decisions and events. We present a novel approach for LDA-pre-processing called Iterative Filtering to study such phenomena based on Twitter data. In combination with Hashtag Pooling as an additional pre-processing step, we are able to achieve a coherent framing of the discussion and topics of interest, despite of the inherent noisiness and sparseness of Twitter data. Our novel approach enables researchers to gain detailed insights into discourses of interest on Twitter, allowing them to identify tweets iteratively that are related to an investigated topic of interest. As an application, we study the dynamics of conspiracy-related topics on US Twitter during the last four months of 2020, which were dominated by the US-Presidential Elections and Covid-19. We monitor the public discourse in the USA with geo-spatial Twitter data to identify conspiracy-related contents by estimating Latent Dirichlet Allocation (LDA) Topic Models. We find that in this period, usual conspiracy-related topics played a marginal role in comparison with dominating topics, such as the US-Presidential Elections or the general discussions about Covid-19. The main conspiracy theories in this period were the ones linked to “Election Fraud” and the “Covid-19-hoax.” Conspiracy-related keywords tended to appear together with Trump-related words and words related to his presidential campaign.}
}

@article{thormann2021stock, title={Stock Price Predictions with LSTM Neural Networks and Twitter Sentiment}, volume={9}, url={http://www.iapress.org/index.php/soic/article/view/1202}, DOI={10.19139/soic-2310-5070-1202}, abstractNote={&lt;div class=&quot;page&quot; title=&quot;Page 1&quot;&gt; &lt;div class=&quot;layoutArea&quot;&gt; &lt;div class=&quot;column&quot;&gt; &lt;p&gt;Predicting the trend of stock prices is a central topic in financial engineering. Given the complexity and nonlinearity of the underlying processes we consider the use of neural networks in general and sentiment analysis in particular for the analysis of financial time series. As one of the biggest social media platforms with a user base across the world, Twitter offers a huge potential for such sentiment analysis. In fact, stocks themselves are a popular topic in Twitter discussions. Due to the real-time nature of the collective information quasi contemporaneous information can be harvested for the prediction of financial trends. In this study, we give an introduction in financial feature engineering as well as in the architecture of a Long Short-Term Memory (LSTM) to tackle the highly nonlinear problem of forecasting stock prices. This paper presents a guide for collecting past tweets, processing for sentiment analysis and combining them with technical financial indicators&lt;br&gt;to forecast the stock prices of Apple 30m and 60m ahead. A LSTM with lagged close price is used as a baseline model. We are able to show that a combination of financial and Twitter features can outperform the baseline in all settings. The code to fully replicate our forecasting approach is available in the Appendix.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&#38;gt;}, number={2}, journal={Statistics, Optimization &amp; Information Computing}, author={Thormann, Marah-Lisanne and Farchmin, Jan and Weisser, Christoph and Kruse, Rene-Marcel and Säfken, Benjamin and Silbersdorff, Alexander}, year={2021}, month={May}, pages={268-287} }

@InProceedings{luber2021identifying,
author="Luber, Mattias
and Weisser, Christoph
and S{\"a}fken, Benjamin
and Silbersdorff, Alexander
and Kneib, Thomas
and Kis-Katos, Krisztina",
editor="Bright, Jonathan
and Giachanou, Anastasia
and Spaiser, Viktoria
and Spezzano, Francesca
and George, Anna
and Pavliuc, Alexandra",
title="Identifying Topical Shifts in Twitter Streams: An Integration of Non-negative Matrix Factorisation, Sentiment Analysis and Structural Break Models for Large Scale Data",
booktitle="Disinformation in Open Online Media",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="33--49",
abstract="We propose an integration of Non-negative Matrix Factorisation, Sentiment analysis and Structural Break Models to identify significant topical shifts on the social media platform Twitter. For the topic modelling, we compare Latent Dirichlet Allocation and Non-negative Matrix Factorization in terms of their applicability to short text documents. The extraction of sentiment is done by the rule-based VADER model. Structural breaks in the relative frequency and daily sentiments of topics over time are identified with the Bai-Perron model. Combining these methods, we provide a valuable and easy to use exploratory tool for social scientists to study the discourse on Twitter over time. Detecting statistically significant shifts in topics over time enables researchers to perform statistical inference and test hypotheses about the discourse on Twitter. The framework is implemented efficiently to ensure that it can be used on average consumer hardware in a reasonable amount of time. A case study with COVID-19 related tweets in the UK is provided. Our method is validated by linking the topical shifts to real world events by the use of the timestamps of the COVID-19 related tweets.",
isbn="978-3-030-87031-7",
url = {https://doi.org/10.1007/978-3-030-87031-7_3},
}

@article{kant2024oneway,
  author    = {Gillian Kant and Ivan Zhelyazkov and Anton Thielmann and Christoph Weisser and Michael Schlee and Christoph Ehrling and Benjamin S{\"a}fken and Thomas Kneib},
  title     = {One-way ticket to the moon? An NLP-based insight on the phenomenon of small-scale neo-broker trading},
  journal   = {Social Network Analysis and Mining},
  year      = {2024},
  volume    = {14},
  number    = {1},
  pages     = {121},
  doi       = {10.1007/s13278-024-01273-2},
  url       = {https://doi.org/10.1007/s13278-024-01273-2},
  issn      = {1869-5469},
  abstract  = {We present an Natural Language Processing based analysis on the phenomenon of “Meme Stocks”, which has emerged as a result of the proliferation of neo-brokers like Robinhood and the massive increase in the number of small-scale stock investors. Such investors often use specific Social Media channels to share short-term investment decisions and strategies, resulting in partial collusion and planning of investment decisions. The impact of online communities on the stock prices of affected companies has been considerable in the short term. This paper has two objectives. Firstly, we chronologically model the discourse on the most prominent platforms. Secondly, we examine the potential for using collaboratively made investment decisions as a means to assist in the selection of potential investments.. To understand the investment decision-making processes of small-scale investors, we analyze data from Social Media platforms like Reddit, Stocktwits and Seeking Alpha. Our methodology combines Sentiment Analysis and Topic Modelling. Sentiment Analysis is conducted using VADER and a fine-tuned BERT model. For Topic Modelling, we utilize LDA, NMF and the state-of-the-art BERTopic. We identify the topics and shapes of discussions over time and evaluate the potential for leveraging information of the decision-making process of investors for trading choices. We utilize Random Forest and Neural Network Models to show that latent information in discussions can be exploited for trend prediction of stocks affected by Social Network driven herd behavior. Our findings provide valuable insights into content and sentiment of discussions and are a vehicle to improve efficient trading decisions for stocks affected from short-term herd behavior.}
}

@book{goodfellow2016deep,
  title={Deep Learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  publisher={MIT Press},
  year={2016},
}